{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c0e0c-3106-46ee-b8e6-c31c0975d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "import seaborn as sns\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "from pyspark.sql.functions import *\n",
    "import random\n",
    "\n",
    "# Setting up visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1120f99-3898-4b22-b9e3-d6601058926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    \"Voters_Gender\", # cat\n",
    "    \"Voters_Age\", # num\n",
    "#    \"Voters_BirthDate\", # ignore\n",
    "    \"Residence_Families_HHCount\", # num\n",
    "    \"Residence_HHGender_Description\", # cat\n",
    "    \"Mailing_Families_HHCount\", # num\n",
    "    \"Mailing_HHGender_Description\", # cat\n",
    "\n",
    "#   !! voter party affiliation\n",
    "    \"Parties_Description\", \n",
    "    \n",
    "    # cat\n",
    "    \"CommercialData_PropertyType\",\n",
    "    \"AddressDistricts_Change_Changed_CD\",\n",
    "    \"AddressDistricts_Change_Changed_SD\",\n",
    "    \"AddressDistricts_Change_Changed_HD\",\n",
    "    \"AddressDistricts_Change_Changed_County\",\n",
    "    \n",
    "    \"Residence_Addresses_Density\", # num\n",
    "    \n",
    "    # cat\n",
    "    \"CommercialData_EstimatedHHIncome\",\n",
    "    \"CommercialData_ISPSA\",\n",
    "#    \"CommercialData_AreaMedianEducationYears\",\n",
    "    \"CommercialData_AreaMedianHousingValue\",\n",
    "    \"CommercialData_MosaicZ4Global\",\n",
    "#     \"CommercialData_AreaPcntHHMarriedCoupleNoChild\",  ## Redundant\n",
    "#     \"CommercialData_AreaPcntHHMarriedCoupleWithChild\",\n",
    "#     \"CommercialData_AreaPcntHHSpanishSpeaking\",\n",
    "#     \"CommercialData_AreaPcntHHWithChildren\",\n",
    "    \"CommercialData_StateIncomeDecile\",\n",
    "#    \"Ethnic_Description\",\n",
    "    \"EthnicGroups_EthnicGroup1Desc\",\n",
    "    \"CommercialData_DwellingType\",\n",
    "    \"CommercialData_PresenceOfChildrenCode\",\n",
    "#    \"CommercialData_PresenceOfPremCredCrdInHome\", ## too many missing\n",
    "    \"CommercialData_DonatesToCharityInHome\",\n",
    "    \"CommercialData_DwellingUnitSize\",\n",
    "    \"CommercialData_ComputerOwnerInHome\",\n",
    "    \"CommercialData_DonatesEnvironmentCauseInHome\",\n",
    "    \"CommercialData_Education\",\n",
    "    \n",
    "#   Don't include because of lookahead bias  \n",
    "#     \"Voters_VotingPerformanceEvenYearGeneral\",\n",
    "#     \"Voters_VotingPerformanceEvenYearPrimary\",\n",
    "#     \"Voters_VotingPerformanceEvenYearGeneralAndPrimary\",\n",
    "#     \"Voters_VotingPerformanceMinorElection\",\n",
    "    \n",
    "#   Other control variables that expect to be highly associated with outcome:\n",
    "#     \"ElectionReturns_P08CountyTurnoutAllRegisteredVoters\",\n",
    "#     \"ElectionReturns_P08CountyTurnoutDemocrats\",\n",
    "#     \"ElectionReturns_P08CountyTurnoutRepublicans\",\n",
    "    \"General_2000\",\n",
    "    \"General_2004\",\n",
    "    \"PresidentialPrimary_2000\",\n",
    "    \"PresidentialPrimary_2004\",\n",
    "        \n",
    "#   Outcome variable (indiana law happens in 2005, approved by SCOTUS before presidential election in 2008)\n",
    "    \"General_2008\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b9cbc-1d10-44e3-93a0-649e524540c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the states that do not have strict voter ID laws:\n",
    "#  'VM2Uniform--CA--2021-05-02',\tVM2Uniform--CA--2021-05-02\tCA\tx\tCalifornia\n",
    "#  'VM2Uniform--IL--2021-03-05',\tVM2Uniform--IL--2021-03-05\tIL\tx\tIllinois\n",
    "#  'VM2Uniform--MA--2021-01-19',\tVM2Uniform--MA--2021-01-19\tMA\tx\tMassachusetts\n",
    "#  'VM2Uniform--MD--2021-02-15',\tVM2Uniform--MD--2021-02-15\tMD\tx\tMaryland\n",
    "#  'VM2Uniform--ME--2021-05-28',\tVM2Uniform--ME--2021-05-28\tME\tx\tMaine\n",
    "#  'VM2Uniform--MN--2021-02-14',\tVM2Uniform--MN--2021-02-14\tMN\tx\tMinnesota\n",
    "#  'VM2Uniform--NC--2021-05-18',\tVM2Uniform--NC--2021-05-18\tNC\tx\tNorth Carolina\n",
    "#  'VM2Uniform--NE--2021-01-20',\tVM2Uniform--NE--2021-01-20\tNE\tx\tNebraska\n",
    "#  'VM2Uniform--NJ--2021-03-11',\tVM2Uniform--NJ--2021-03-11\tNJ\tx\tNew Jersey\n",
    "#  'VM2Uniform--NM--2021-02-25',\tVM2Uniform--NM--2021-02-25\tNM\tx\tNew Mexico\n",
    "#  'VM2Uniform--NV--2021-06-13',\tVM2Uniform--NV--2021-06-13\tNV\tx\tNevada\n",
    "#  'VM2Uniform--NY--2021-03-15',\tVM2Uniform--NY--2021-03-15\tNY\tx\tNew York\n",
    "#  'VM2Uniform--OR--2021-02-05',\tVM2Uniform--OR--2021-02-05\tOR\tx\tOregon\n",
    "#  'VM2Uniform--PA--2021-05-20',\tVM2Uniform--PA--2021-05-20\tPA\tx\tPennsylvania\n",
    "#  'VM2Uniform--VT--2021-05-28',\tVM2Uniform--VT--2021-05-28\tVT\tx\tVermont\n",
    "\n",
    "# For each of these states, I want to pull enough samples to get a total sample of 1/2 M; can increase later\n",
    "\n",
    "# grab files\n",
    "states =  [\n",
    "# For now, just exclude New York and Califonria, because the parquet files take too long to read\n",
    " 'VM2Uniform--VT--2021-05-28',\n",
    "    \n",
    "    \n",
    "    \n",
    "#   'VM2Uniform--IL--2021-03-05',\n",
    "#   'VM2Uniform--MA--2021-01-19',\n",
    "#   'VM2Uniform--MD--2021-02-15',\n",
    "#   'VM2Uniform--ME--2021-05-28',\n",
    "#   'VM2Uniform--MN--2021-02-14',\n",
    "#   'VM2Uniform--NC--2021-05-18',\n",
    "#   'VM2Uniform--NE--2021-01-20',\n",
    "#   'VM2Uniform--NJ--2021-03-11',\n",
    "#   'VM2Uniform--NM--2021-02-25',\n",
    "#   'VM2Uniform--NV--2021-06-13',\n",
    "#   'VM2Uniform--OR--2021-02-05',\n",
    "#   'VM2Uniform--PA--2021-05-20',\n",
    " \n",
    "    \n",
    "     #'VM2Uniform--CA--2021-05-02',\n",
    "\n",
    "     #'VM2Uniform--NY--2021-03-15',\n",
    "\n",
    "]\n",
    "\n",
    "# bucket file path for all state parquet files\n",
    "gcs_path = 'gs://pstat135-voter-file/VM2Uniform'\n",
    "\n",
    "# create list of state abbreviations\n",
    "pattern = re.compile(r\"(?<=--)[A-Z]{2}\")\n",
    "state_abvs = re.findall(pattern, ''.join(states))\n",
    "\n",
    "# do first iteration\n",
    "print('VM2Uniform--VT--2021-05-28')\n",
    "\n",
    "num_per_state = 100000\n",
    "\n",
    "df_ref = spark.read.parquet(\"/\".join([gcs_path, 'VM2Uniform--VT--2021-05-28']))\n",
    "df_ref = df_ref.select(cols_to_keep)\n",
    "\n",
    "numrows = {'VM2Uniform--VT--2021-05-28': df_ref.count()}\n",
    "\n",
    "print(\"%d\" % (numrows['VM2Uniform--VT--2021-05-28']))\n",
    "    \n",
    "percentage_sample = num_per_state / numrows['VM2Uniform--VT--2021-05-28']\n",
    "    \n",
    "df_ref = df_ref.sample(True, percentage_sample, seed = 19480384)\n",
    "df_ref = df_ref.withColumn('STATE', F.lit(state_abvs[0]))\n",
    " \n",
    "next_states = states[1:]\n",
    "\n",
    "# do the rest of the iterations\n",
    "for i, one_state in enumerate(next_states):\n",
    "\n",
    "    print(\"%s: \" % (one_state), end=\"\")\n",
    "    \n",
    "    # read dataframe for one_state\n",
    "    tmp_ref = spark.read.parquet(\"/\".join([gcs_path, one_state]))\n",
    "    tmp_ref = tmp_ref.select(cols_to_keep)\n",
    "    numrows[one_state] = tmp_ref.count()\n",
    "    print(\"%d\" % (numrows[one_state]))\n",
    "    \n",
    "    percentage_sample = num_per_state / numrows[one_state]\n",
    "    \n",
    "    tmp_ref = tmp_ref.sample(True, percentage_sample, seed = 19480384)\n",
    "    tmp_ref = tmp_ref.withColumn('STATE', F.lit(state_abvs[i+1]))\n",
    "    \n",
    "    df_ref = df_ref.union(tmp_ref)      \n",
    "\n",
    "# gcs_path = 'gs://pstat135-voter-file/VM2Uniform'\n",
    "# numrows = dict()\n",
    "# first DF\n",
    "#df_ref = spark.read.parquet('gs://pstat135-voter-file/VM2Uniform/')\n",
    "df_ref.printSchema()\n",
    "df_ref.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef336d8-4729-4341-a01a-7fecc7580962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# others\n",
    "for one_state in states[1:]:\n",
    "    print(\"%s: \" % (one_state), end=\"\")\n",
    "    \n",
    "    # read dataframe for one_state\n",
    "    df = spark.read.parquet(\"/\".join([gcs_path, one_state]))\n",
    "    \n",
    "    # sample small proportion\n",
    "    sample_df = df.sample(True, percentage_sample, seed = 19480384)\n",
    "    \n",
    "    # Then, rbind / concatenate all the files together\n",
    "    df_ref = df_ref.union(sample_df)\n",
    "\n",
    "# Save as parquet into google cloud\n",
    "\n",
    "df_ref.write.format(\"parquet\").save(\"total_reference_sample\")\n",
    "df_ref = spark.read.parquet(\"total_reference_sample\")\n",
    "\n",
    "# Then, run the Indiana file's code on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71f1d3-acba-4634-a776-6470d8400031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for categorical variables, replace missing value with \"O\" for \n",
    "# other (including General_2000, General_2004, PresidentialPrimary_2000, PresidentialPrimary_2004)\n",
    "\n",
    "# Get a list of all column types:\n",
    "numeric_cols = [item[0] for item in df_ref.dtypes if not (item[1].startswith('string') or item[1].startswith('date'))]\n",
    "categorical_cols = [item[0] for item in df_ref.dtypes if item[1].startswith('string')]\n",
    "\n",
    "categorical_cols\n",
    "#numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7bb56-dcd9-4b44-b085-8081f2721cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref.select('CommercialData_DwellingType',\n",
    " 'CommercialData_PresenceOfChildrenCode',\n",
    " 'CommercialData_DonatesToCharityInHome',\n",
    " 'CommercialData_DwellingUnitSize').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde8e4a-f869-425b-89cb-9866b4778e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    'Voters_Age',\n",
    "    'Residence_Families_HHCount',\n",
    "    'Mailing_Families_HHCount',\n",
    "    'Residence_Addresses_Density'\n",
    "]\n",
    "\n",
    "trinary_cols = [\n",
    "    'CommercialData_DonatesToCharityInHome',\n",
    "    'CommercialData_ComputerOwnerInHome',\n",
    "    'CommercialData_DonatesEnvironmentCauseInHome'\n",
    "]\n",
    "\n",
    "binary_cols = [\n",
    "    \"General_2000\",\n",
    "    \"General_2004\",\n",
    "    \"PresidentialPrimary_2000\",\n",
    "    \"PresidentialPrimary_2004\"\n",
    "]\n",
    "\n",
    "other_cols = [c for c in df_ref.columns if c != \"General_2008\"]\n",
    "other_cols = [c for c in other_cols if c not in (numeric_cols + trinary_cols + binary_cols)]\n",
    "\n",
    "categorical_cols = other_cols + binary_cols + trinary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680085e-7c64-472b-953f-05ed26b313c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in numeric_cols:\n",
    "    df_ref = df_ref.withColumn(c, F.col(c).cast(\"float\").alias(c))\n",
    "df_ref = df_ref.fillna(\"U\", subset= trinary_cols)\n",
    "df_ref = df_ref.fillna(\"Missing\", subset = other_cols)\n",
    "df_ref = df_ref.fillna(\"N\", subset = binary_cols)\n",
    "df_ref.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b6e99-8b06-4ec1-8be2-c39fb7618c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create copy of working df\n",
    "new_df = df_ref.alias('new_df')\n",
    "\n",
    "# Impute the missing values in the numerical columns with the mean -- minimize change to z-scores of given data\n",
    "imputer = Imputer(\n",
    "    inputCols=numeric_cols, \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in numeric_cols]\n",
    ")\n",
    "\n",
    "new_df = imputer.fit(new_df).transform(new_df)\n",
    "\n",
    "# Impute categorical columns -- maybe it's better to drop these records\n",
    "# new_df = new_df.fillna(\"missing\", subset = categorical_cols)\n",
    "\n",
    "# Assemble numerical columns into vector\n",
    "vec = VectorAssembler(inputCols = [c+\"_imputed\" for c in numeric_cols], outputCol = \"numFeatures\")\n",
    "new_df = vec.transform(new_df)\n",
    "\n",
    "# Standardize the numeric columns\n",
    "scaler = StandardScaler(inputCol=\"numFeatures\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "new_df = scaler.fit(new_df).transform(new_df)\n",
    "\n",
    "# output columns to be indexed\n",
    "indexed_cols = [f\"index{i}\" for i in range(len(categorical_cols))]\n",
    "\n",
    "# Ecode categorical variables\n",
    "indexer = StringIndexer(inputCols = categorical_cols, outputCols = indexed_cols)\n",
    "new_df = indexer.fit(new_df).transform(new_df)\n",
    "\n",
    "# Through categorical variables in their own vector\n",
    "vec = VectorAssembler(inputCols = indexed_cols, outputCol = \"catFeatures\")\n",
    "new_df = vec.transform(new_df)\n",
    "\n",
    "# Concatenate scaled numerical and categorical vectors into single features vector\n",
    "vec = VectorAssembler(inputCols = ['scaledFeatures', 'catFeatures'], outputCol = \"features\")\n",
    "new_df = vec.transform(new_df)\n",
    "\n",
    "new_df.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52eace-b368-4108-82e8-07a0c52721a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.groupBy('General_2004').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400cf87-b516-4320-8026-eaac0e05621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indi.groupBy(\"General_2004\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a261b56-6c2f-415c-830d-591c92e588f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "2734476/(2734476 + 1662100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb404344-e938-466f-8ef5-e60b3c693af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "336427/(162862+336427)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8fa085-08a3-47ee-8af5-db25b4ea67e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "indi = spark.read.parquet(\"gs://voter-project-235-25/VM2Uniform--IN--2021-01-15_parq\")\n",
    "indi = indi.sample(True, 0.1, seed = 19480384)\n",
    "\n",
    "# clean the total parquet file--------------\n",
    "\n",
    "num_voters_indiana = indi.count()\n",
    "num_voters_not_indiana = df_ref.count()\n",
    "pct_sample = num_voters_indiana / num_voters_not_indiana\n",
    "\n",
    "# empty list to store the estimated average treatment effects:\n",
    "ATEs = []\n",
    "\n",
    "# empty dictionary to store output\n",
    "stored_DF = {}\n",
    "\n",
    "i = 0\n",
    "for i in range(1000):\n",
    "    i = i + 1 # from 1 - 100 rather than 0 to 100\n",
    "\n",
    "    # take random sample of the total parquet file (equivalent to the size of indiana)\n",
    "    df_ref = df_ref.sample(True, pct_sample, seed = i)\n",
    "\n",
    "    # create dummy DATA:\n",
    "    indi = indi.withColumn('Voters_Age', rand())\n",
    "    df_ref = df_ref.withColumn('Voters_Age', rand())\n",
    "    \n",
    "    indi = indi.select([\"Voters_Age\"])\n",
    "    df_ref = df_ref.select([\"Voters_Age\"])\n",
    "\n",
    "    indi = indi.withColumn('Voters_Age', col('Voters_Age').cast('double'))\n",
    "    df_ref = df_ref.withColumn('Voters_Age', col('Voters_Age').cast('double'))\n",
    "\n",
    "    indi = indi.withColumn('General_2008_RANDOM', when(rand() > 0.5, 1).otherwise(0))\n",
    "    df_ref = df_ref.withColumn('General_2008_RANDOM', when(rand() > 0.5, 1).otherwise(0))\n",
    "\n",
    "    # create a column with \"LAW == 0\" for non-Indiana states\n",
    "    df_ref = df_ref.withColumn(\"LAW\", lit(0))\n",
    "\n",
    "    # create a column with \"LAW == 1\" for Indiana\n",
    "    indi = indi.withColumn(\"LAW\", lit(1))\n",
    "\n",
    "    # union the two together\n",
    "    df = df_ref.union(indi)\n",
    "\n",
    "    df_input_logistic = df.drop(\"General_2008_RANDOM\")\n",
    "\n",
    "    # fit logistic model on the intervention (variable Law)\n",
    "    nrow_df_input_logistic_start_check = df_input_logistic.count()\n",
    "    df_input_logistic = df_input_logistic.fillna(0)\n",
    "    df_input_logistic.columns\n",
    "    supervised = RFormula(formula=\"LAW ~ .\")\n",
    "    fittedRF = supervised.fit(df_input_logistic) # inspect column types\n",
    "    prepareddf_input_logistic = fittedRF.transform(df_input_logistic) # create feature and label columns\n",
    "    prepareddf_input_logistic.show(5, truncate = False)\n",
    "    lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")\n",
    "    print(lr.explainParams())\n",
    "    lrModel = lr.fit(prepareddf_input_logistic) # train model\n",
    "    lrModel.transform(prepareddf_input_logistic).select(\"label\", \"prediction\") # fitted values\n",
    "    plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "    beta = np.sort(lrModel.coefficients)\n",
    "    plt.plot(beta)\n",
    "    plt.ylabel('Beta Coefficients')\n",
    "\n",
    "    # get a propensity score from the probability as a new column:\n",
    "    fitted = lrModel.transform(prepareddf_input_logistic)\n",
    "    fitted = fitted.withColumn('probability', vector_to_array('probability'))\n",
    "    array_mean = udf(lambda x: float(np.mean(x)), FloatType())\n",
    "    fitted = fitted.withColumn(\"propensity_score\", array_mean(\"probability\"))\n",
    "\n",
    "    fitted.count() == nrow_df_input_logistic_start_check\n",
    "\n",
    "    # new column that called weight that is T - PS / (PS * 1 - PS)\n",
    "    fitted = fitted.withColumn(\"weight\", (col(\"label\") - col(\"propensity_score\")) / (col(\"propensity_score\") * (1-col(\"propensity_score\"))))\n",
    "\n",
    "    # merge back in the 2008 general election OUTCOME data\n",
    "    fitted = fitted.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "    df = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "    num_row_df_prior = df.count()\n",
    "    df = df.join(fitted, [\"row_id\", \"Voters_Age\", \"LAW\"]).drop(\"row_id\")\n",
    "    num_row_df_prior == df.count()\n",
    "\n",
    "    # calculate the weighted average\n",
    "    df = df.withColumn(\"weighted_outcome\", col(\"General_2008_RANDOM\") * col(\"weight\"))\n",
    "\n",
    "    # store weighted average into list\n",
    "    ATE_this_round = df.agg(avg(col(\"weighted_outcome\"))).collect()[0][0]\n",
    "    ATEs.append(ATE_this_round)\n",
    "\n",
    "    # store the DataFrame into a dictionary\n",
    "    stored_DF[f\"{i}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1813c-fc52-481e-a87b-22eb67637825",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25100b03-b9a7-4c73-a73a-b9d12f281ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bdc650-3cbb-4cb4-8c27-465c4f1296c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0610db-dfe5-46ee-921b-f9932a1ef24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b409b0f-98ce-4bda-821b-9a976b23fed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a12a2-66a4-4e95-821d-dca7869f1312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c171e6-936f-4d5d-970a-23557919ad26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966685c-e485-454f-8fc5-7cb0f34f2412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06fde65-78ee-408c-8e59-c404a3716e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591eed83-65e5-4b33-98de-5d68fd071d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create box plot showing propensity score by treated or not treated\n",
    "# divy this vizualization up by states (?)\n",
    "\n",
    "# sns.boxplot(x=\"success_expect\", y=\"propensity_score\", data=data_ps)\n",
    "# plt.title(\"Confounding Evidence\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}