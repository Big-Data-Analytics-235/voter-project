{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d753d16-e679-400a-ac53-17a341fa1608",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c0e0c-3106-46ee-b8e6-c31c0975d403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "import seaborn as sns\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, FloatType\n",
    "from pyspark.sql.functions import *\n",
    "import random\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, Imputer, StringIndexer\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import RFormula\n",
    "import time\n",
    "\n",
    "# Setting up visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a222e23-7414-4f9c-9869-37de6c9419f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    \"Voters_Gender\", # cat\n",
    "#     \"Voters_Age\", # num\n",
    "    \"Voters_BirthDate\", # ignore\n",
    "    \"Residence_Families_HHCount\", # num\n",
    "    \"Residence_HHGender_Description\", # cat\n",
    "    \"Mailing_Families_HHCount\", # num\n",
    "    \"Mailing_HHGender_Description\", # cat\n",
    "\n",
    "#   !! voter party affiliation\n",
    "    \"Parties_Description\", \n",
    "    \n",
    "    # cat\n",
    "    \"CommercialData_PropertyType\",\n",
    "    \"AddressDistricts_Change_Changed_CD\",\n",
    "    \"AddressDistricts_Change_Changed_SD\",\n",
    "    \"AddressDistricts_Change_Changed_HD\",\n",
    "    \"AddressDistricts_Change_Changed_County\",\n",
    "    \n",
    "    \"Residence_Addresses_Density\", # num\n",
    "    \n",
    "    # cat\n",
    "    \"CommercialData_EstimatedHHIncome\",\n",
    "    \"CommercialData_ISPSA\",\n",
    "    # num\n",
    "    \"CommercialData_AreaMedianEducationYears\",\n",
    "    \"CommercialData_AreaMedianHousingValue\",\n",
    "#    \"CommercialData_MosaicZ4Global\",\n",
    "    # cat\n",
    "     \"CommercialData_AreaPcntHHMarriedCoupleNoChild\",  \n",
    "     \"CommercialData_AreaPcntHHMarriedCoupleWithChild\",\n",
    "     \"CommercialData_AreaPcntHHSpanishSpeaking\",\n",
    "     \"CommercialData_AreaPcntHHWithChildren\",\n",
    "     \"CommercialData_StateIncomeDecile\",\n",
    "#    \"Ethnic_Description\",\n",
    "    \"EthnicGroups_EthnicGroup1Desc\",\n",
    "    \"CommercialData_DwellingType\",\n",
    "    \"CommercialData_PresenceOfChildrenCode\",\n",
    "#    \"CommercialData_PresenceOfPremCredCrdInHome\", ## too many missing\n",
    "    \"CommercialData_DonatesToCharityInHome\",\n",
    "    \"CommercialData_DwellingUnitSize\",\n",
    "    \"CommercialData_ComputerOwnerInHome\",\n",
    "    \"CommercialData_DonatesEnvironmentCauseInHome\",\n",
    "    \"CommercialData_Education\",\n",
    "    \n",
    "#   Don't include because of lookahead bias  \n",
    "#     \"Voters_VotingPerformanceEvenYearGeneral\",\n",
    "#     \"Voters_VotingPerformanceEvenYearPrimary\",\n",
    "#     \"Voters_VotingPerformanceEvenYearGeneralAndPrimary\",\n",
    "#     \"Voters_VotingPerformanceMinorElection\",\n",
    "    \n",
    "#   Other control variables that expect to be highly associated with outcome:\n",
    "#     \"ElectionReturns_P08CountyTurnoutAllRegisteredVoters\",\n",
    "#     \"ElectionReturns_P08CountyTurnoutDemocrats\",\n",
    "#     \"ElectionReturns_P08CountyTurnoutRepublicans\",\n",
    "    \"General_2000\",\n",
    "    \"General_2004\",\n",
    "    \"PresidentialPrimary_2000\",\n",
    "    \"PresidentialPrimary_2004\",\n",
    "        \n",
    "#   Outcome variable (indiana law happens in 2005, approved by SCOTUS before presidential election in 2008)\n",
    "    \"General_2008\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b9cbc-1d10-44e3-93a0-649e524540c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are the states that do not have strict voter ID laws:\n",
    "#  'VM2Uniform--CA--2021-05-02',\tVM2Uniform--CA--2021-05-02\tCA\tx\tCalifornia\n",
    "#  'VM2Uniform--IL--2021-03-05',\tVM2Uniform--IL--2021-03-05\tIL\tx\tIllinois\n",
    "#  'VM2Uniform--MA--2021-01-19',\tVM2Uniform--MA--2021-01-19\tMA\tx\tMassachusetts\n",
    "#  'VM2Uniform--MD--2021-02-15',\tVM2Uniform--MD--2021-02-15\tMD\tx\tMaryland\n",
    "#  'VM2Uniform--ME--2021-05-28',\tVM2Uniform--ME--2021-05-28\tME\tx\tMaine\n",
    "#  'VM2Uniform--MN--2021-02-14',\tVM2Uniform--MN--2021-02-14\tMN\tx\tMinnesota\n",
    "#  'VM2Uniform--NC--2021-05-18',\tVM2Uniform--NC--2021-05-18\tNC\tx\tNorth Carolina\n",
    "#  'VM2Uniform--NE--2021-01-20',\tVM2Uniform--NE--2021-01-20\tNE\tx\tNebraska\n",
    "#  'VM2Uniform--NJ--2021-03-11',\tVM2Uniform--NJ--2021-03-11\tNJ\tx\tNew Jersey\n",
    "#  'VM2Uniform--NM--2021-02-25',\tVM2Uniform--NM--2021-02-25\tNM\tx\tNew Mexico\n",
    "#  'VM2Uniform--NV--2021-06-13',\tVM2Uniform--NV--2021-06-13\tNV\tx\tNevada\n",
    "#  'VM2Uniform--NY--2021-03-15',\tVM2Uniform--NY--2021-03-15\tNY\tx\tNew York\n",
    "#  'VM2Uniform--OR--2021-02-05',\tVM2Uniform--OR--2021-02-05\tOR\tx\tOregon\n",
    "#  'VM2Uniform--PA--2021-05-20',\tVM2Uniform--PA--2021-05-20\tPA\tx\tPennsylvania\n",
    "#  'VM2Uniform--VT--2021-05-28',\tVM2Uniform--VT--2021-05-28\tVT\tx\tVermont\n",
    "\n",
    "# For each of these states, I want to pull enough samples to get a total sample of 1/2 M; can increase later\n",
    "\n",
    "# grab files\n",
    "states =  [\n",
    "# For now, just exclude New York and Califonria, because the parquet files take too long to read\n",
    "'VM2Uniform--VT--2021-05-28', \n",
    "'VM2Uniform--IL--2021-03-05',\n",
    "'VM2Uniform--MA--2021-01-19',\n",
    "'VM2Uniform--MD--2021-02-15',\n",
    "'VM2Uniform--ME--2021-05-28',\n",
    "'VM2Uniform--MN--2021-02-14',\n",
    "'VM2Uniform--NC--2021-05-18',\n",
    "'VM2Uniform--NE--2021-01-20',\n",
    "'VM2Uniform--NJ--2021-03-11',\n",
    "'VM2Uniform--NM--2021-02-25',\n",
    "'VM2Uniform--NV--2021-06-13',\n",
    "'VM2Uniform--OR--2021-02-05',\n",
    "'VM2Uniform--PA--2021-05-20',\n",
    "'VM2Uniform--CA--2021-05-02',\n",
    "'VM2Uniform--NY--2021-03-15',\n",
    "]\n",
    "\n",
    "# bucket file path for all state parquet files\n",
    "gcs_path = 'gs://pstat135-voter-file/VM2Uniform'\n",
    "\n",
    "# create list of state abbreviations\n",
    "pattern = re.compile(r\"(?<=--)[A-Z]{2}\")\n",
    "state_abvs = re.findall(pattern, ''.join(states))\n",
    "\n",
    "# do first iteration\n",
    "print('VM2Uniform--VT--2021-05-28')\n",
    "\n",
    "# num_per_state = 500\n",
    "\n",
    "df_ref = spark.read.parquet(\"/\".join([gcs_path, 'VM2Uniform--VT--2021-05-28']))\n",
    "df_ref = df_ref.select(cols_to_keep)\n",
    "\n",
    "numrows = {'VM2Uniform--VT--2021-05-28': df_ref.count()}\n",
    "\n",
    "print(\"%d\" % (numrows['VM2Uniform--VT--2021-05-28']))\n",
    "    \n",
    "# percentage_sample = num_per_state / numrows['VM2Uniform--VT--2021-05-28']\n",
    "    \n",
    "# df_ref = df_ref.sample(True, percentage_sample, seed = 19480384)\n",
    "df_ref = df_ref.withColumn('STATE', F.lit(state_abvs[0]))\n",
    " \n",
    "next_states = states[1:]\n",
    "\n",
    "# do the rest of the iterations\n",
    "for i, one_state in enumerate(next_states):\n",
    "\n",
    "    print(\"%s: \" % (one_state), end=\"\")\n",
    "    \n",
    "    # read dataframe for one_state\n",
    "    tmp_ref = spark.read.parquet(\"/\".join([gcs_path, one_state]))\n",
    "    tmp_ref = tmp_ref.select(cols_to_keep)\n",
    "    numrows[one_state] = tmp_ref.count()\n",
    "    print(\"%d\" % (numrows[one_state]))\n",
    "    \n",
    "#     percentage_sample = num_per_state / numrows[one_state]\n",
    "    \n",
    "#     tmp_ref = tmp_ref.sample(True, percentage_sample, seed = 19480384)\n",
    "    tmp_ref = tmp_ref.withColumn('STATE', F.lit(state_abvs[i+1]))\n",
    "    \n",
    "    df_ref = df_ref.union(tmp_ref)      \n",
    "\n",
    "df_ref.printSchema()\n",
    "df_ref.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817211b7-c20f-4104-9cb3-599bb1e818cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cc248f4-cb77-4bd9-b108-6cc201c91b91",
   "metadata": {},
   "source": [
    "### FUNCTIONS TO CLEAN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dfa990-c35e-4f57-8fa3-5eb03f7e5fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_numeric_categorical(input_df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    # remove special symbols ($, %) from relevant columns\n",
    "    input_df = input_df.withColumn(\n",
    "        \"CommercialData_AreaMedianHousingValue\",\n",
    "        F.expr(\"substring(CommercialData_AreaMedianHousingValue, 2, length(CommercialData_AreaMedianHousingValue))\"))\n",
    "\n",
    "    pct = [\"CommercialData_AreaPcntHHMarriedCoupleNoChild\",  \n",
    "           \"CommercialData_AreaPcntHHMarriedCoupleWithChild\",\n",
    "           \"CommercialData_AreaPcntHHSpanishSpeaking\",\n",
    "           \"CommercialData_AreaPcntHHWithChildren\"]\n",
    "\n",
    "    for c in pct:\n",
    "        input_df = input_df.withColumn(\n",
    "            c,\n",
    "            F.expr(f\"substring({c}, 1, length({c})-1)\")\n",
    "        )\n",
    "    input_df.select([\"CommercialData_AreaMedianHousingValue\"]+pct).show()\n",
    "\n",
    "    numeric_cols = [\n",
    "        'Residence_Families_HHCount',\n",
    "        'Mailing_Families_HHCount',\n",
    "        'Residence_Addresses_Density',\n",
    "        \"CommercialData_AreaMedianEducationYears\",\n",
    "        \"CommercialData_AreaMedianHousingValue\"\n",
    "    ] + pct\n",
    "\n",
    "    trinary_cols = [\n",
    "        'CommercialData_DonatesToCharityInHome',\n",
    "        'CommercialData_ComputerOwnerInHome',\n",
    "        'CommercialData_DonatesEnvironmentCauseInHome'\n",
    "    ]\n",
    "\n",
    "    binary_cols = []\n",
    "\n",
    "    dont_touch_cols = [\n",
    "        \"General_2008\", \n",
    "        \"Voters_BirthDate\", \n",
    "        \"General_2000\",\n",
    "        \"General_2004\",\n",
    "        \"PresidentialPrimary_2000\",\n",
    "        \"PresidentialPrimary_2004\"\n",
    "    ]\n",
    "    \n",
    "    other_cols = [c for c in input_df.columns if c not in dont_touch_cols]\n",
    "    other_cols = [c for c in other_cols if c not in (numeric_cols + trinary_cols + binary_cols)]\n",
    "\n",
    "    categorical_cols = other_cols + binary_cols + trinary_cols\n",
    "\n",
    "    for c in numeric_cols:\n",
    "        input_df = input_df.withColumn(c, F.col(c).cast(\"float\").alias(c))\n",
    "    input_df = input_df.fillna(\"U\", subset= trinary_cols)\n",
    "    input_df = input_df.fillna(\"Missing\", subset = other_cols)\n",
    "#   input_df = input_df.fillna(\"N\", subset = binary_cols)\n",
    "    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec695ff1-2c8e-48c1-b6ac-e3c03f660ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = clean_numeric_categorical(df_ref)\n",
    "df_ref.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71f1d3-acba-4634-a776-6470d8400031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def impute_values_function(input_df: DataFrame) -> DataFrame:\n",
    "\n",
    "    # Create copy of working df\n",
    "    input_df = input_df.alias('input_df')\n",
    "\n",
    "    # Impute the missing values in the numerical columns with the mean -- minimize change to z-scores of given data\n",
    "    imputer = Imputer(\n",
    "        inputCols=numeric_cols, \n",
    "        outputCols=[\"{}_imp\".format(c) for c in numeric_cols]\n",
    "    )\n",
    "\n",
    "    input_df = imputer.fit(input_df).transform(input_df)\n",
    "\n",
    "    # Impute categorical columns -- maybe it's better to drop these records\n",
    "    # input_df = input_df.fillna(\"missing\", subset = categorical_cols)\n",
    "\n",
    "    indexed_cols = [f\"{c}_ind\" for c in categorical_cols]\n",
    "\n",
    "    # Ecode categorical variables\n",
    "    indexer = StringIndexer(inputCols = categorical_cols, outputCols = indexed_cols)\n",
    "    input_df = indexer.fit(input_df).transform(input_df)\n",
    "    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d4243-b1c7-4d40-8e4e-2ef892ac2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = impute_values_function(df_ref)\n",
    "df_ref.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6583f33-e168-4822-bb6d-26939d3117ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref.select(\"Voters_BirthDate\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36bfdf-a9fb-4e69-b410-e28badae9f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_voter_participation(input_df: DataFrame) -> DataFrame:\n",
    "\n",
    "    yrs_add = 18\n",
    "    months_add = 18*12\n",
    "\n",
    "    # date of national \n",
    "    target_month_day_presidential = \"11-03\"\n",
    "\n",
    "    # date of presidential primary (ideally we should do this state by state, but this is the date for Indiana's)\n",
    "    target_month_day_primary = \"05-03\" \n",
    "\n",
    "    input_df = input_df.withColumn(\"DATE_18\", add_months(to_date(col(\"Voters_BirthDate\"),\"MM/dd/yyyy\"), months_add))\n",
    "    input_df.select([\"Voters_BirthDate\", \"DATE_18\"]).show(10)\n",
    "    input_df = input_df.dropna(subset = \"Voters_BirthDate\")\n",
    "    input_df = input_df.withColumn(\"YEAR_18\", year(\"DATE_18\"))\n",
    "    input_df = input_df.withColumn(\"comparator_date_presidential\", to_date(concat(col(\"YEAR_18\"), lit(\"-\"), lit(target_month_day_presidential))))\n",
    "    input_df = input_df.withColumn(\"comparator_date_primary\", to_date(concat(col(\"YEAR_18\"), lit(\"-\"), lit(target_month_day_primary))))\n",
    "\n",
    "    for election in [\"PRESIDENTIAL\", \"PRIMARY\"]:\n",
    "        input_df = input_df.withColumn(f\"YEAR_ELIGIBLE_TO_VOTE_{election}\", \\\n",
    "                                    when(col(\"DATE_18\")<=col(f\"comparator_date_{election.lower()}\"), col(\"YEAR_18\")) \\\n",
    "                                   .otherwise(col(\"YEAR_18\") + 1) \\\n",
    "                                  )\n",
    "\n",
    "    # check no missing vals:\n",
    "    input_df.where(col(\"YEAR_18\").isNull()).select(\"YEAR_18\").show(10)\n",
    "\n",
    "    # get rid of rows where the voter was not old enough to vote in the 2008 general election\n",
    "    input_df = input_df.filter(col(\"YEAR_ELIGIBLE_TO_VOTE_PRESIDENTIAL\")<=2008).fillna(\"N\", subset = [\"General_2008\"])\n",
    "\n",
    "    # for the 2000 and 2004 general elections, replace with \"N\" IF the person was old enough to vote at the time\n",
    "\n",
    "    for election in [\"2000\", \"2004\"]:\n",
    "        input_df = input_df.withColumn(f\"General_{election}\", \\\n",
    "                                   when((col(\"YEAR_ELIGIBLE_TO_VOTE_PRESIDENTIAL\")<= int(election)) & \\\n",
    "                                        (col(f\"General_{election}\").isNull()), \"N\") \\\n",
    "                                   .otherwise(col(f\"General_{election}\")) \\\n",
    "                                  )\n",
    "\n",
    "        input_df = input_df.withColumn(f\"PresidentialPrimary_{election}\", \\\n",
    "                                   when((col(\"YEAR_ELIGIBLE_TO_VOTE_PRIMARY\")<= int(election)) & \\\n",
    "                                        (col(f\"PresidentialPrimary_{election}\").isNull()), \"N\") \\\n",
    "                                   .otherwise(col(f\"PresidentialPrimary_{election}\")) \\\n",
    "                                  )\n",
    "\n",
    "    # make the general voting for 2008 a numeric variable; since we've deleted\n",
    "    # everyone who was not eligible to vote, this can be directly calculated with a 1-0.\n",
    "    input_df = input_df.withColumn(\"Voted_General_2008\", when(input_df.General_2008 == \"Y\",1).otherwise(0))\n",
    "    input_df = input_df.drop(\"General_2008\")\n",
    "    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffc4c7-5719-4921-8631-2f7bf11d8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = clean_voter_participation(df_ref)\n",
    "df_ref.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a7958-eadd-4b14-81aa-fbb6ca4c863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indi.select(\"Voters_BirthDate\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac33037-8adf-4b87-8359-30c5d3b37dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do the same process for Indiana:\n",
    "indi = spark.read.parquet(\"gs://voter-project-235-25/VM2Uniform--IN--2021-01-15_parq\")\n",
    "indi = indi.sample(True, 0.1, seed = 19480384)\n",
    "indi = indi.select(cols_to_keep)\n",
    "indi = indi.withColumn(\"STATE\", lit(\"IN\"))\n",
    "\n",
    "indi = clean_numeric_categorical(indi)\n",
    "indi = impute_values_function(indi)\n",
    "indi = clean_voter_participation(indi)\n",
    "indi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a5227-64af-49e7-9fa6-7f4df467d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "indi.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66ee9d-8b16-4ef1-b7de-44c5ca5420d0",
   "metadata": {},
   "source": [
    "# Estimator with logistic regression index model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96997cec-1761-4777-a9a1-537a7da57561",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.groupBy('General_2004').count().show()\n",
    "\n",
    "indi_full = spark.read.parquet(\"gs://voter-project-235-25/VM2Uniform--IN--2021-01-15_parq\")\n",
    "indi = indi_full.select('General_2004')\n",
    "\n",
    "indi.groupBy(\"General_2004\").count().show()\n",
    "\n",
    "new_df = new_df.fillna(\"N\", subset = \"General_2008\")\n",
    "\n",
    "indexer = StringIndexer(inputCol = \"General_2008\", outputCol = \"label\")\n",
    "new_df = indexer.fit(new_df).transform(new_df)\n",
    "\n",
    "new_df.select(\n",
    "    [\"{}_imp\".format(c) for c in numeric_cols] + indexed_cols + [\"label\"]    \n",
    ").printSchema()\n",
    "\n",
    "new_df = new_df.select(\n",
    "    [\"{}_imp\".format(c) for c in numeric_cols] + indexed_cols + [\"label\"]    \n",
    ")\n",
    "\n",
    "from pyspark.ml.feature import RFormula\n",
    "supervised = RFormula(formula=\"label ~ .\")\n",
    "\n",
    "fittedRF = supervised.fit(new_df)\n",
    "\n",
    "preparedDF = fittedRF.transform(new_df)\n",
    "\n",
    "preparedDF.select(\"features\").show(n=10, truncate=False)\n",
    "\n",
    "train, test = new_df.randomSplit([0.7, 0.3], seed = 42069)\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "featureCols = pd.DataFrame(preparedDF.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"nominal\"]+\n",
    "  preparedDF.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"numeric\"]).sort_values(\"idx\")\n",
    "\n",
    "featureCols = featureCols.set_index('idx')\n",
    "featureCols.head()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "\n",
    "beta = np.sort(lrModel.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "\n",
    "coefsArray = np.array(lrModel.coefficients)  # convert to np.array\n",
    "coefsDF = pd.DataFrame(coefsArray, columns=['coefs'])  # to pandas\n",
    "\n",
    "coefsDF = coefsDF.merge(featureCols, left_index=True, right_index=True)  # join it with featureCols we created above\n",
    "coefsDF.sort_values('coefs', inplace=True)  # Sort them\n",
    "coefsDF.head()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(coefsDF.name, coefsDF.coefs)\n",
    "plt.title('Ranked coefficients from the logistic regression model')\n",
    "plt.show()\n",
    "\n",
    "df_ref.write.format(\"parquet\").save(\"total_reference_sample\")\n",
    "df_ref = spark.read.parquet(\"total_reference_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aec9f61-168b-432a-8c90-d646e8fd7476",
   "metadata": {},
   "source": [
    "# Propensity score estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8fa085-08a3-47ee-8af5-db25b4ea67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_voters_indiana = indi.count()\n",
    "num_voters_not_indiana = df_ref.count()\n",
    "pct_sample = num_voters_indiana / num_voters_not_indiana\n",
    "\n",
    "# empty list to store the estimated average treatment effects:\n",
    "ATEs = []\n",
    "\n",
    "# empty dictionary to store output\n",
    "stored_DF = {}\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "for i in range(1):\n",
    "    i = i + 1 # from 1 - 100 rather than 0 to 100\n",
    "\n",
    "    # take random sample of the total parquet file (equivalent to the size of indiana)\n",
    "    df_ref = df_ref.sample(True, pct_sample, seed = i)\n",
    "\n",
    "    # create dummy DATA:\n",
    "#     indi = indi.withColumn('Voters_Age', rand())\n",
    "#     df_ref = df_ref.withColumn('Voters_Age', rand())\n",
    "    \n",
    "#     indi = indi.select([\"Voters_Age\"])\n",
    "#     df_ref = df_ref.select([\"Voters_Age\"])\n",
    "\n",
    "#     indi = indi.withColumn('Voters_Age', col('Voters_Age').cast('double'))\n",
    "#     df_ref = df_ref.withColumn('Voters_Age', col('Voters_Age').cast('double'))\n",
    "\n",
    "#     indi = indi.withColumn('General_2008_RANDOM', when(rand() > 0.5, 1).otherwise(0))\n",
    "#     df_ref = df_ref.withColumn('General_2008_RANDOM', when(rand() > 0.5, 1).otherwise(0))\n",
    "\n",
    "    # create a column with \"LAW == 0\" for non-Indiana states\n",
    "    df_ref = df_ref.withColumn(\"LAW\", lit(0))\n",
    "\n",
    "    # create a column with \"LAW == 1\" for Indiana\n",
    "    indi = indi.withColumn(\"LAW\", lit(1))\n",
    "\n",
    "    # union the two together\n",
    "    df = df_ref.union(indi)\n",
    "\n",
    "    cols_excluded_from_regression = [\n",
    "        'Voters_BirthDate', # removed this, but KEPT the YEAR that the voter turned 18.\n",
    "        'STATE',\n",
    "        'STATE_ind',\n",
    "        'DATE_18',\n",
    "        'comparator_date_presidential',\n",
    "        'comparator_date_primary',\n",
    "        'YEAR_ELIGIBLE_TO_VOTE_PRESIDENTIAL',\n",
    "        'YEAR_ELIGIBLE_TO_VOTE_PRIMARY',\n",
    "        'Voted_General_2008'\n",
    "    ]\n",
    "    \n",
    "    df_input_logistic = df.drop(*cols_excluded_from_regression)\n",
    "\n",
    "    # fit logistic model on the intervention (variable Law)\n",
    "    nrow_df_input_logistic_start_check = df_input_logistic.count()\n",
    "#     df_input_logistic = df_input_logistic.fillna(0)\n",
    "    df_input_logistic.columns\n",
    "    supervised = RFormula(formula=\"LAW ~ .\")\n",
    "    fittedRF = supervised.fit(df_input_logistic) # inspect column types\n",
    "    prepareddf_input_logistic = fittedRF.transform(df_input_logistic) # create feature and label columns\n",
    "    prepareddf_input_logistic.show(5, truncate = False)\n",
    "    lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")\n",
    "    print(lr.explainParams())\n",
    "    lrModel = lr.fit(prepareddf_input_logistic) # train model\n",
    "    lrModel.transform(prepareddf_input_logistic).select(\"label\", \"prediction\") # fitted values\n",
    "\n",
    "    # get a propensity score from the probability as a new column:\n",
    "    fitted = lrModel.transform(prepareddf_input_logistic)\n",
    "    fitted = fitted.withColumn('probability', vector_to_array('probability'))\n",
    "    array_mean = udf(lambda x: float(np.mean(x)), FloatType())\n",
    "    fitted = fitted.withColumn(\"propensity_score\", array_mean(\"probability\"))\n",
    "\n",
    "    fitted.count() == nrow_df_input_logistic_start_check\n",
    "\n",
    "    # new column that called weight that is T - PS / (PS * 1 - PS)\n",
    "    fitted = fitted.withColumn(\"weight\", (col(\"label\") - col(\"propensity_score\")) / (col(\"propensity_score\") * (1-col(\"propensity_score\"))))\n",
    "\n",
    "    # merge back in the 2008 general election OUTCOME data\n",
    "    fitted = fitted.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "    df = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "    num_row_df_prior = df.count()\n",
    "    df = df.join(fitted, [\"row_id\", \"Voters_Age\", \"LAW\"]).drop(\"row_id\")\n",
    "    num_row_df_prior == df.count()\n",
    "\n",
    "    # calculate the weighted average\n",
    "    df = df.withColumn(\"weighted_outcome\", col(\"General_2008_RANDOM\") * col(\"weight\"))\n",
    "\n",
    "    # store weighted average into list\n",
    "    ATE_this_round = df.agg(avg(col(\"weighted_outcome\"))).collect()[0][0]\n",
    "    ATEs.append(ATE_this_round)\n",
    "\n",
    "    # store the DataFrame into a dictionary\n",
    "    stored_DF[f\"{i}\"] = df\n",
    "    \n",
    "# end_time = time.time()\n",
    "# print(\"Execution time: {:.2f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1813c-fc52-481e-a87b-22eb67637825",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25100b03-b9a7-4c73-a73a-b9d12f281ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48471afe-455b-4d3b-9412-1a218db6eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "range(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0cf4fb-db85-4954-bf02-cb3d169a5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "print(\"Execution time: {:.2f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a3df5-6311-4694-8b6e-de843af44054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b9c5ac-e621-41ff-80b4-2569efdeaf0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed973943-8854-42e3-89ab-56cf7fe03e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec0404-e9ac-4bfb-8eb2-74ef9db8b87c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}