{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b28054-4af3-4738-b79c-dd228e99c35f",
   "metadata": {},
   "source": [
    "# Analysis of Voter Turnout in Indiana Pre- and Post- Voter Identification Law\n",
    "### Authors: Christopher Lefrak, Hannah Li, George Yang, and Kuai Yu\n",
    "### PSTAT 235\n",
    "\n",
    "NOTES/TO-DO (delete this before submitting):\n",
    "- truncate/limit code so the writeup looks polished and professional (no raw outputs/errors)\n",
    "- interpret findings\n",
    "- include visualizations and graphs (EDA? theoretical concepts?)\n",
    "- run and write ONLY NECESSARY code (label all code clearly and eliminate commented out code) in the \"Final Code file.ipynb\" file, save all visualizations as images to embedd in this writeup document so it's more organized.\n",
    "- submission: \"Writeup.ipynb\" file and \"Final Code file.ipynb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a2d3b-0c12-4ebc-a7db-91ad51fefd29",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[importance/potential effect of voter ID law]\n",
    "\n",
    "Thirty-five of the fifty states of the U.S. have passed stricter voter ID laws that require or request voters to present a form of identification at the polls. \n",
    "The remaining fifteen states do not require voters to present any documentation to vote at the polls. States such as Indiana, Wisconsin, and Tennessee have strict photo ID laws for voters, while states such as Minnesota, Nebraska, North Carolina, and Pennsylvania have no requirements for voter identification. A visualization of the levels of strictness of voter photo identification laws for each state can be seen in the map in **Fig. 1** below.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1-bKVXaRl_j3trfCus6iWYylx3RX4ACPz\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> Fig. 1: Strictness Levels of US Voter ID Laws </figcaption>\n",
    "</figure>\n",
    "\n",
    "Advantages of implementing stricter voter identification requirements include preventing voter impersonation, thus  increasing public confidence in election processes. Disadvantages of implementing stricter laws unnecessarily burdens voters and administrators.\n",
    "\n",
    "## Goals\n",
    "In this project, we conduct a case study to see if implementing laws that inhibit the convenience of voting can ultimately lead to decreased voter participation. We focus our investigations of voter identification laws on the state of Indiana, which implemented a strict voter identification law in 2008. We seek to analyze how much voter turnout would have decreased or increased without the implentation of the law. \n",
    "\n",
    "> Project Goals\n",
    "> - Conduct logistic regression\n",
    "> - Propensity score \n",
    "> - Doubly Robust Estimation\n",
    "> - Strengthen our PySpark data analysis skills, collaborative skills, and project organization skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff4d48-f0d5-44ff-ad6c-638ef44b5a47",
   "metadata": {},
   "source": [
    "## Voter Data\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "Our voter data is obtained from the course's VM2Uniform folder. We primarily use the dataset corresponding to Indiana. At a glance, the dataset contains 726 columns and 946908 rows, records beginning from .... and ending in 2021\n",
    "\n",
    "[eda/visualizations]\n",
    "\n",
    "We subsetted the dataset to focus on a narrower set of voter attributes. The columns we selected from the original dataset can be seen in the following section.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706cd871-ea95-426f-9a37-369f076189f3",
   "metadata": {},
   "source": [
    "### Input Variables\n",
    "In the table below we display the columns we keep from the original dataset\n",
    "\n",
    "| Column Name | Data Type | Values |\n",
    "| --- | --- | --- |\n",
    "| Voters_Gender | categorical | 'F', 'M', 'Missing' |\n",
    "| Voters_BirthDate | --- | format 99/99/9999 |\n",
    "| Residence_Families_HHCount | numerical | 1 through 10 |\n",
    "| Residence_HHGender_Description | categorical | 'Cannot Determine', 'Female Only Household', 'Male Only Household', 'Mixed Gender Household' |\n",
    "| Mailing_Families_HHCount | numerical |  ... |\n",
    "| Mailing_HHGender_Description | categorical |  'Cannot Determine', 'Female Only Household', 'Male Only Household', 'Mixed Gender Household' |\n",
    "| Parties_Description | categorical | 'Republican', 'Unknown', 'Other', 'Democratic', 'Non-Partisan' |\n",
    "| CommercialData_PropertyType | categorical | 'Apartment', 'Mobil Home', 'Residential', 'Unknown', 'Condominium', 'Missing', 'Triplex', 'Duplex' |\n",
    "| AddressDistricts_Change_Changed_CD | categorical | 'Between 6 Months and 1 Year Ago', 'Has Not Changed Within Last 2 Years', 'Between 1 and 2 Years Ago', 'Within Last 6 Months' |\n",
    "| AddressDistricts_Change_Changed_SD | categorical | 'Between 6 Months and 1 Year Ago', 'Has Not Changed Within Last 2 Years', 'Between 1 and 2 Years Ago', 'Within Last 6 Months' |\n",
    "| AddressDistricts_Change_Changed_HD | categorical | 'Between 6 Months and 1 Year Ago', 'Has Not Changed Within Last 2 Years', 'Between 1 and 2 Years Ago', 'Within Last 6 Months' |\n",
    "| AddressDistricts_Change_Changed_County | categorical | 'Between 6 Months and 1 Year Ago', 'Has Not Changed Within Last 2 Years', 'Between 1 and 2 Years Ago', 'Within Last 6 Months' |\n",
    "| Residence_Addresses_Density | numerical | ... |\n",
    "| CommercialData_EstimatedHHIncome | categorical | '175000-199999', '250000+', '1000-14999', '100000-124999', '75000-99999', '125000-149999', '25000-34999', '200000-249999', '50000-74999', '150000-174999', '35000-49999', '15000-24999', 'Unknown' |\n",
    "| CommercialData_ISPSA | categorical | None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 |\n",
    "| CommercialData_AreaMedianEducationYears | numerical |  ... |\n",
    "| CommercialData_AreaMedianHousingValue | numerical | ... |\n",
    "| CommercialData_AreaPcntHHMarriedCoupleNoChild | categorical | ... |\n",
    "| CommercialData_AreaPcntHHMarriedCoupleWithChild | categorical | ... |\n",
    "| CommercialData_AreaPcntHHSpanishSpeaking | categorical | ... |\n",
    "| CommercialData_AreaPcntHHWithChildren | categorical | ... |\n",
    "| CommercialData_StateIncomeDecile | categorical | None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 |\n",
    "| EthnicGroups_EthnicGroup1Desc | categorical |  'East and South Asian', 'Eurpoean', 'Hispanic and Portuguese', 'Likely African-American', 'Missing', 'Other' |\n",
    "| CommercialData_DwellingType | categorical | 'Multi-Family Dwelling', 'Missing', 'Single Family Dwelling Unit' |\n",
    "| CommercialData_PresenceOfChildrenCode | categorical | 'Modeled Likely to have a child', 'Not Likely to have a child', 'Modeled Not as Likely to have a child', 'Known Data', 'Missing' |\n",
    "| CommercialData_DonatesToCharityInHome | categorical | 'Y', 'U' |\n",
    "| CommercialData_DwellingUnitSize | categorical | '5-9', '2-Duplex', '50-100', '1-Single Family Dwelling', '10-19', 'Missing', '4', '101+', '3-Triplex', '20-49' |\n",
    "| CommercialData_ComputerOwnerInHome | categorical | 'Y', 'U' |\n",
    "| CommercialData_DonatesEnvironmentCauseInHome | categorical | 'Y', 'U' |\n",
    "| CommercialData_Education | categorical | 'Grad Degree - Extremely Likely', 'Grad Degree - Likely', 'Bach Degree - Extremely Likely', 'HS Diploma - Likely', 'Less than HS Diploma - Ex Like', 'Some College - Likely', 'Vocational Technical Degree - Extremely Likely', 'Some College -Extremely Likely', 'HS Diploma - Extremely Likely', 'Less than HS Diploma - Likely', 'Bach Degree - Likely', 'Missing' |\n",
    "\n",
    "\n",
    "### Other Variables\n",
    "The table below shows other control variables that we expect to be highly associated with the response variable.\n",
    "\n",
    "| Column Name | Data Type | Values |\n",
    "| --- | --- | --- |\n",
    "| General_2000 | categorical | Y/null |\n",
    "| General_2004 | categorical | Y/null |\n",
    "| PresidentialPrimary_2000 | categorical | Y/null |\n",
    "| PresidentialPrimary_2004 | categorical | Y/null |\n",
    "\n",
    "\n",
    "The table below shows the response variable.\n",
    "\n",
    "| Column Name | Data Type | Values |\n",
    "| --- | --- | --- |\n",
    "| General_2008 | categorical | Y/null |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2c95a-63d7-4dfc-9ced-29d69e89abb0",
   "metadata": {},
   "source": [
    "### Other States\n",
    "In the table below are states that do not have strict voter identification laws.\n",
    "\n",
    "| State | Abbreviation |\n",
    "| --- | --- |\n",
    "| California | CA | \n",
    "| Illinois | IL |\n",
    "| Massachusetts | MA | \n",
    "| Maryland | MD | \n",
    "| Maine | ME |\n",
    "| Minnesota | MN | \n",
    "| North Carolina | NC | \n",
    "| Nebraska | NE |\n",
    "| New Jersey | NJ | \n",
    "| New Mexico | NM | \n",
    "| Nevada | NV |\n",
    "| New York | NY | \n",
    "| Oregon | OR | \n",
    "| Pennsylvania | PA |\n",
    "| Vermont | VT | \n",
    "\n",
    "**Note:** Throughout this report, when we refer to \"other states\", we mean all states from this list above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562385eb-ce20-4531-a9b4-136523d766c1",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Many of the columns contain symbols including `$` and `%`, so we remove those symbols.\n",
    "\n",
    "Many columns are also missing data. In numerical columns, we impute these values in with the mean value to minimize any changes to z-scores of the given data. This is desirable because we are going to standardize our values when implement our machine learning algorithms. We don't want to throw away valuable data and discard the nulls entirely, but we also don't want our imputations to artifically influence/inflate the effect that the predictors may have with the response.\n",
    "\n",
    "Many of the categorical variables in the dataset already have an \"Unknown\" level. As far as we are concerned, if the value is missing, then the value is unknown. Though, this is different for the case that voter participation data may be missing for a given election if the given individual is not old enough to have voted in that election.\n",
    "\n",
    "Our data consists of registered voters as of 2021, so it also contains many individuals who were not old enough to vote in the 2008 general election (they were below the 18 year old age requirement). We removed the rows corresponding to these voters. \n",
    "We then converted the `General_2008` variable to be numerical. \n",
    "\n",
    "When fitting our models in PySpark, we used `RFormula` to create a `features` column that is a vector of all relevant predictors (numerical and categorical)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91518760-88b2-4acd-93fa-984aa686d3cb",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a statistical method that allows us to estimate the probability that an event occurs, in this case, if an individual voted or not, given a set of independent variables $X_1, X_2,...,X_k$. If the $i$th individual has a probability $\\pi_i$ of voting, then the model is that the log odds of voting, $\\ln (\\frac{\\pi_i}{1-\\pi_i})$, can be expressed as a linear combination of the covariates $X_{i1}, X_{i2},...,X_{ik}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ln(\\frac{\\pi_i}{1-\\pi_i})&=\\beta_0+\\beta_1 X_{i1}+\\beta_2 X_{i2}+...\\beta_k X_{ik}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The coefficients of this function are $\\beta_0, \\beta_1, \\beta_2,\\dots, \\beta_k$, and indicate the relative effect of the corresponding variables $X_1,X_2,...,X_k$ on the response variable. The larger the coefficient $\\beta_j$, the more a varying value of $X_j$ changes the log-odds. The optimal coefficients maximize the function to find the best fit.\n",
    "\n",
    "Of course, this idea of logistic regression generalize to more than just voting; it applies whenever our concerned response variable is binary.\n",
    "\n",
    "Throughout this report, we split our dataset into 70% training data and 30% testing data, and fit logistic regression on our training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6577fec-7b46-4a02-956d-81323467486f",
   "metadata": {},
   "source": [
    "## Model 1 - Logistic Regression on Voter Turnout\n",
    "\n",
    "For a first attempt in identitfying if Indiana's voter turnout in 2008 was affected by the strict voter ID law that was implemented during that year, we first fit a logistic regression to the voter turnout variable `General_2008`. In particular, we trained the model to predict whether a given individual voted during the 2008 election. However, the model was only given data from states without voter ID laws to learn from, i.e., all of our working states except for Indiana.\n",
    "\n",
    "The idea is that, if Indiana's law was key in shaping voter behavior of Indiana residents in 2008, then our model would have a poorer prediction accuracy when predicting voter turnout in Indiana than compared to a test set from the states without the same identification requirements.\n",
    "\n",
    "#### Resulting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4039a0-af05-4306-a2af-dfd83ce67514",
   "metadata": {},
   "source": [
    "**Fig. 2** shows a lineplot of the sorted array of the resulting $\\beta$ coefficients against their index in the array. We notice that there are around 120 coefficients, but we only used around 37 predictors. The reason for these extra coefficients is because many of our 37 predictors are categorical. We need to be able to turn our categorical predictors into numbers, so one categorical variables gets converted into many variables. For example, If there are five distinct values for a categorical variable, it will be converted into 4 different indicators, with the 5 corresponding to $\\beta_0$.\n",
    "\n",
    "We can see that a majority of these predictors had coefficients close to 0, indicating that many of these predictors are not useful in determining voter turnout.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1nHp-AuG_SA0MpNHA0AmFNKE4dmSbYFzN\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> Fig. 2: Logistic Regression Beta Coefficients </figcaption>\n",
    "</figure>\n",
    "\n",
    "**Fig. 3** shows all of the predictor's with significantly large coefficients. I.e., predictors that have a significant role in predicting whether or not a given individual voted in the General 2008 election. Since many coefficients had coefficients that were nearly zero, the figure only displays predictors whose associated coefficient has a magnitude larger than 0.25. This allows us to read the names of the predictors that have meaningful correlation with voter turnout.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1N7jCemVQwY1DYzseHOW690yiw99Jc7l4\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> Fig. 3: Logistic Regression Ranked Coefficients </figcaption>\n",
    "</figure>\n",
    "\n",
    "Perhaps unsurprisingly, it seems that an individual's party description plays a big role in their likelihood of voting. We see that more a lot of progressive or liberal-esque party description resulted in a lower odds of voting in the 2008 election. However, we see that those who identified with the \"Women's Equality party\" as well as African Americans had an increased odds of voting. This could be due to the historical implications the 2008 election had on the civil rights movement since it involved the first ever African American presidential candidate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b6e53-113c-43a9-acb8-438b2e2399c1",
   "metadata": {},
   "source": [
    "# The Performance\n",
    "\n",
    "The summary of the performance metrics of the model is given in the table below\n",
    "\n",
    "| Metric | Other State | Indiana |\n",
    "| --- | --- | --- |\n",
    "| Overall Accuracy | 79.2% | 78.6% |\n",
    "| False Positive Rate | 13.3% | 14.9% |\n",
    "| False Negative Rate | 29.2% | 28.4% |\n",
    "| Area Under ROC | 86.2% | 86.0% |\n",
    "\n",
    "Overall, every metric between the control test set and the Indiana data are very comparable. It was verified that the model's performance was similar on the training data itself, indicating that overfitting was not an issue. \n",
    "\n",
    "Since the model could predict whether an individual from Indiana voted in the 2008 election just as well as it could predict someone voting in another state, it may seem that the voter ID law is unimportant in determinine voter turnout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11734024-a001-421e-85de-a420cf7a5b1e",
   "metadata": {},
   "source": [
    "## Propensity Score\n",
    "Our goal is to predict if a voter has passed the law or not.\n",
    "\n",
    "We want to be able to find the probability if someone votes if they did not have the voter identification law implented.\n",
    "\n",
    "Assumption: People outside of Indiana are representative of the people in indiana.\n",
    "\n",
    "\n",
    "> Variables:\n",
    "> - T = whether they have the law\n",
    "> - Y = whether they voted in 2008\n",
    "> - P = predicted T\n",
    "\n",
    "To compare the voter data with and without the implementation of a voter identification law, we observe that the difference in means $$E[Y|T=1]-E[Y|T=0]$$ is [...]. Thus, the treated voters (with implementation of a voter identification law) have a [...] compared to non-treated voters.\n",
    "\n",
    "The propensity score is the conditional probability of receiving the treatment, the implementation of the voter identification law. Using this score means that we do not have to achieve conditional independence $(Y_1,Y_0) \\perp |X$. In other words, we do not have to condition on the whole $X$ to achieve independence of potential outcomes of the treatment. Instead, it is sufficient to control confounders $X$ for a propensity score $$P(x)=P(T|X)=E[T|X]$$ to achieve $(Y_1,Y_0) \\perp |P(x)$.\n",
    "\n",
    "The propensity score essentially converts $X$ into the treatment $T$, acting as a middleman between $X$ and $T$. Initially, we cannot compare treated and non-treated osbervations. However, we can compare a treated and a non-treated observation if they have the same probability of receiving the treatment since receiving or not receiving the treatment would be attributed to randomness. Thus, we hold the propensity score constant to make the data appear more random.\n",
    "\n",
    "\n",
    "### Propensity Weighting and Estimation\n",
    "\n",
    "We write the difference in means again, but we now condition on $X$: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& E[Y|X,T=1]-E[Y|X,T=0]\\\\\n",
    "& =E[\\frac{Y}{P(x)}|X,T=1]P(x)-E[\\frac{Y}{(1-P(x))}|X,T=0](1-P(x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In other words, the propensity score more heavily weights the observations with a low probability of receiving treatment, and weakly weights...\n",
    "\n",
    "We simplify our propensity score weighting estimator to $E[Y\\frac{T-P(x)}{P(x)(1-P(x)}]$, where $P(x)$ and $(1-P(x))$ both must be greater than $0$. Thus, each voter must have some probability of both receiving and not receiving the treatment of the implementation of a voter identification law.\n",
    "\n",
    "\n",
    "We now estimate the true propensity score $P(x)$ with $\\hat{P}(x)$ using logistic regression.\n",
    "\n",
    "> Issues\n",
    "> - The propensity score's strong predictive power can hurt our goals of causal inference.\n",
    ">> - We want out prediction to control for confounding variables, not neccesarily to predict the treatment very well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf3407-9697-4e57-a3a8-91f11602a054",
   "metadata": {},
   "source": [
    "## Doubly Robust Estimation\n",
    "\n",
    "So far we have used logistic regression to directly model voter turnout, and we have used propensity score weighting to estimate $E[Y|T=1] - E[Y|T=0] | X$ (i.e., the expected difference in liklihood for an Indianan vs other state resident to vote in the 2008). However, we can combine the advantages of both techniques into a single estimator known as the *doubly robust estimator*. The estimator for the average treatment effect is given by\n",
    "\n",
    "$$\\hat{ATE} = \\frac{1}{N}\\sum \\bigg( \\dfrac{T_i(Y_i - \\hat{\\mu_1}(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu_1}(X_i) \\bigg) - \\frac{1}{N}\\sum \\bigg( \\dfrac{(1-T_i)(Y_i - \\hat{\\mu_0}(X_i))}{1-\\hat{P}(X_i)} + \\hat{\\mu_0}(X_i) \\bigg)$$\n",
    "\n",
    "where $\\hat{\\mu_0}(x)$ is an estimate to $E[Y| X, T=0]$ (probability someone with given quality votes given they are from another state), $\\hat{\\mu_1}(x)$ is an estimate to $E[Y| X, T=1]$ (probability someone with given quality votes given they are from Indiana), and $\\hat{P}(x)$ is an estimation of the propensity score.\n",
    "\n",
    "In implementation, we can think of these quantities as follows:\n",
    "\n",
    "- $\\hat{\\mu_0}(X_i)$: predicted probabilities of voting for all voters given a model is trained on other states.\n",
    "- $\\hat{\\mu_1}(X_i)$: predicted probabilities of voting for all voters given a model is trained on just Indiana.\n",
    "- $\\hat{P}(X_i)$: predicted probabilities of having a voter ID law for all voters given a model is trained on all states.\n",
    "\n",
    "In theory, this estimator would combine both the previous methods to obtain a number between -1 and 1 which would indicate the expected difference in probabilities between voters in Indiana and other states.\n",
    "\n",
    "Unfortunately, in implementation, our initial run gave us a value of $\\hat{ATE} = -1.18$ which would indicate that someone in Indiana is expected to be 118% less likely to vote than someone from another state. Of course, this value is nonsensical, indicating that there was some sort of bug in the code most likely. **Fig. 4** shows the distribution of $\\hat{P}(x)$ broken down by whether the state had a voter ID law (Indiana) or not (other states). This model seems to be performing well, so it is curious as to why the estimate is so off. Ideally, it would be good to bootstrap a bunch of samples to get some uncertainty bounds on this estimator, however it is computationally infeasible for our given scenario.\n",
    "\n",
    "However, we did bootstrap twice and got estimates of ~ $-19$ and ~ $+11$, so this method seems to be wildly inaccurate for this application. This is surprising that we have such a large variance given that we have many millions of training records.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Y4Obcx_1xCqewEqJyMOodQ6cWN11GxzQ\" style=\"width:80%\" align = \"center\">\n",
    "<figcaption align = \"center\"> Fig. 4: Distribution of Propensity Scores </figcaption>\n",
    "</figure>\n",
    "\n",
    "There is also some confusing conventions of PySpark's `LogisticRegression` implementation. Typically, the probability $\\pi_i$ that gets predicted is interpreted as the probability that the true value is 1. It seems that PySpark implements this such that $\\pi_i$ is the probability the true value is 0. To add to this confusion, the output of a `LogisticRegression` model has a `probability` column which reports a vector containing both $\\pi_i$ and $1- \\pi_i$. Using either value gives a nonsensical $\\hat{ATE}$, but $-1.18$ is both the more sensical value as well as the value we are more confident was computed correctly.\n",
    "\n",
    "Unfortunately, due to time contrainsts, we are unable to resolve this problem before submission, but it was an interesting topic to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2c481-e077-4f9d-8692-88cf9d0c097d",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "[summary of everything]\n",
    "\n",
    "[issues - curse of dimensionality]\n",
    "\n",
    "[significance]\n",
    "\n",
    "[possible future work]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac20cc5-b91b-455b-be9e-11990075871c",
   "metadata": {},
   "source": [
    "## Resources\n",
    "https://www.ncsl.org/elections-and-campaigns/voter-id#undefined \n",
    "\n",
    "https://matheusfacure.github.io/python-causality-handbook/11-Propensity-Score.html\n",
    "\n",
    "https://www.ibm.com/topics/logistic-regression#:~:text=Resources-,What%20is%20logistic%20regression%3F,given%20dataset%20of%20independent%20variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63204e4b-8a00-47c5-992b-70d8c4cf06a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
