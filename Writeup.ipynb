{"cells": [{"cell_type": "markdown", "id": "a5b28054-4af3-4738-b79c-dd228e99c35f", "metadata": {}, "source": "# Analysis of Voter Turnout in Indiana Pre- and Post- Voter Identification Law\n### Authors: Christopher Lefrak, Hannah Li, George Yang, and Kuai Yu\n### PSTAT 235\n\nNOTES/TO-DO:\n- truncate/limit outputs so the writeup looks polished and professional (no raw outputs/errors)\n- interpret findings\n- include visualizations and graphs (EDA? theoretical concepts?)\n\nUSEFUL jupyter notebook TOOLS:\n- Hide/remove input of a cell\n\n```\n{\n    \"tags\": [\n        \"hide-input\",\n    ]\n}\n```\n\n```\n{\n    \"tags\": [\n        \"remove-input\",\n    ]\n}\n```\n- Hide/remove output of a cell\n\n```\n{\n    \"tags\": [\n        \"hide-output\"\n    ]\n}\n```\n\n```\n{\n    \"tags\": [\n        \"remove-output\",\n    ]\n}\n```\n- Hide/remove input and output of a cell\n```\n{\n    \"tags\": [\n        \"hide-cell\"\n    ]\n}\n```\n\n```\n{\n    \"tags\": [\n        \"remove-cell\"\n    ]\n}\n```"}, {"cell_type": "code", "execution_count": 5, "id": "ba41229e-0aaf-48bc-b112-db5b8f56a306", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Collecting package metadata (current_repodata.json): done\nSolving environment: - \nThe environment is inconsistent, please check the package plan carefully\nThe following packages are causing the inconsistency:\n\n  - conda-forge/linux-64::spyder==5.0.5=py38h578d9bd_2\n  - conda-forge/noarch::cookiecutter==1.7.3=pyh6c4a22f_1\n  - conda-forge/noarch::python-slugify==8.0.1=pyhd8ed1ab_1\ndone\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 4.9.2\n  latest version: 23.1.0\n\nPlease update conda by running\n\n    $ conda update -n base conda\n\n\n\n## Package Plan ##\n\n  environment location: /opt/conda/miniconda3\n\n  added / updated specs:\n    - jupyterlab\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    botocore-1.29.94           |     pyhd8ed1ab_0         5.7 MB  conda-forge\n    zlib-ng-2.0.7              |       h0b41bf4_0          92 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         5.8 MB\n\nThe following packages will be UPDATED:\n\n  alsa-lib                               1.2.3.2-h166bdaf_0 --> 1.2.7.2-h166bdaf_0\n  botocore                             1.29.93-pyhd8ed1ab_0 --> 1.29.94-pyhd8ed1ab_0\n  gst-plugins-base                        1.20.2-hcf0ee16_0 --> 1.20.3-h57caac4_2\n  zlib-ng                                  2.0.6-h166bdaf_0 --> 2.0.7-h0b41bf4_0\n\nThe following packages will be DOWNGRADED:\n\n  pyqt                                5.12.3-py38h578d9bd_8 --> 5.12.3-py38ha8c2ead_4\n\n\n\nDownloading and Extracting Packages\nzlib-ng-2.0.7        | 92 KB     | ##################################### | 100% \nbotocore-1.29.94     | 5.7 MB    | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n\nNote: you may need to restart the kernel to use updated packages.\n"}], "source": "conda update -c conda-forge jupyterlab"}, {"cell_type": "code", "execution_count": 2, "id": "2885aa85-d541-418c-b84f-b3c2c52de3cd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Collecting package metadata (current_repodata.json): done\nSolving environment: | \nThe environment is inconsistent, please check the package plan carefully\nThe following packages are causing the inconsistency:\n\n  - conda-forge/linux-64::spyder==5.0.5=py38h578d9bd_2\n  - conda-forge/noarch::cookiecutter==1.7.3=pyh6c4a22f_1\n  - conda-forge/noarch::python-slugify==8.0.1=pyhd8ed1ab_1\ndone\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 4.9.2\n  latest version: 23.1.0\n\nPlease update conda by running\n\n    $ conda update -n base conda\n\n\n\n## Package Plan ##\n\n  environment location: /opt/conda/miniconda3\n\n  added / updated specs:\n    - jupyterlab\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    alsa-lib-1.2.7.2           |       h166bdaf_0         581 KB  conda-forge\n    bokeh-3.1.0                |     pyhd8ed1ab_0         5.3 MB  conda-forge\n    boto3-1.26.93              |     pyhd8ed1ab_0          76 KB  conda-forge\n    botocore-1.29.93           |     pyhd8ed1ab_0         5.6 MB  conda-forge\n    coverage-7.2.2             |   py38h1de0b5d_0         270 KB  conda-forge\n    exceptiongroup-1.1.1       |     pyhd8ed1ab_0          18 KB  conda-forge\n    filelock-3.10.0            |     pyhd8ed1ab_0          14 KB  conda-forge\n    giflib-5.2.1               |       h0b41bf4_3          76 KB  conda-forge\n    gst-plugins-base-1.20.3    |       h57caac4_2         2.8 MB  conda-forge\n    ipython-8.4.0              |   py38h578d9bd_0         1.1 MB  conda-forge\n    jupyter_latex_envs-1.4.6   |  pyhd8ed1ab_1002         735 KB  conda-forge\n    nss-3.89                   |       he45b914_0         1.9 MB  conda-forge\n    pandoc-3.1.1               |       h32600fe_0        26.2 MB  conda-forge\n    parquet-cpp-1.5.1          |                2           3 KB  conda-forge\n    pathspec-0.11.1            |     pyhd8ed1ab_0          37 KB  conda-forge\n    pexpect-4.8.0              |     pyh1a96a4e_2          48 KB  conda-forge\n    pickleshare-0.7.5          |          py_1003           9 KB  conda-forge\n    platformdirs-3.1.1         |     pyhd8ed1ab_0          17 KB  conda-forge\n    pluggy-1.0.0               |     pyhd8ed1ab_5          16 KB  conda-forge\n    pyqt-5.12.3                |   py38ha8c2ead_4         6.5 MB  conda-forge\n    pyzmq-25.0.1               |   py38he24dcef_0         440 KB  conda-forge\n    qtawesome-1.2.3            |     pyhd8ed1ab_0         1.4 MB  conda-forge\n    qtconsole-5.4.1            |     pyhd8ed1ab_0           7 KB  conda-forge\n    qtconsole-base-5.4.1       |     pyha770c72_0          95 KB  conda-forge\n    requests-futures-1.0.0     |     pyh9f0ad1d_2          10 KB  conda-forge\n    simplejson-3.18.4          |   py38h1de0b5d_0         104 KB  conda-forge\n    snappy-1.1.10              |       h9fff704_0          38 KB  conda-forge\n    terminado-0.15.0           |   py38h578d9bd_0          28 KB  conda-forge\n    thrift_sasl-0.4.3          |     pyhd8ed1ab_2          13 KB  conda-forge\n    typing-extensions-4.5.0    |       hd8ed1ab_0           9 KB  conda-forge\n    typing_extensions-4.5.0    |     pyha770c72_0          31 KB  conda-forge\n    wheel-0.40.0               |     pyhd8ed1ab_0          54 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:        53.5 MB\n\nThe following packages will be UPDATED:\n\n  alsa-lib                               1.2.3.2-h166bdaf_0 --> 1.2.7.2-h166bdaf_0\n  bokeh                                  3.0.3-pyhd8ed1ab_0 --> 3.1.0-pyhd8ed1ab_0\n  boto3                                1.26.87-pyhd8ed1ab_0 --> 1.26.93-pyhd8ed1ab_0\n  botocore                             1.29.87-pyhd8ed1ab_0 --> 1.29.93-pyhd8ed1ab_0\n  coverage                             7.2.1-py38h1de0b5d_0 --> 7.2.2-py38h1de0b5d_0\n  exceptiongroup                         1.1.0-pyhd8ed1ab_0 --> 1.1.1-pyhd8ed1ab_0\n  filelock                               3.9.0-pyhd8ed1ab_0 --> 3.10.0-pyhd8ed1ab_0\n  giflib                                   5.2.1-h36c2ea0_2 --> 5.2.1-h0b41bf4_3\n  gst-plugins-base                        1.20.2-hcf0ee16_0 --> 1.20.3-h57caac4_2\n  jupyter_latex_envs conda-forge/linux-64::jupyter_latex_e~ --> conda-forge/noarch::jupyter_latex_envs-1.4.6-pyhd8ed1ab_1002\n  nss                                       3.88-he45b914_0 --> 3.89-he45b914_0\n  pandoc                                  2.19.2-h32600fe_2 --> 3.1.1-h32600fe_0\n  parquet-cpp        conda-forge/linux-64::parquet-cpp-1.5~ --> conda-forge/noarch::parquet-cpp-1.5.1-2\n  pathspec                              0.11.0-pyhd8ed1ab_0 --> 0.11.1-pyhd8ed1ab_0\n  pexpect            conda-forge/linux-64::pexpect-4.8.0-p~ --> conda-forge/noarch::pexpect-4.8.0-pyh1a96a4e_2\n  pickleshare        conda-forge/linux-64::pickleshare-0.7~ --> conda-forge/noarch::pickleshare-0.7.5-py_1003\n  platformdirs                           3.1.0-pyhd8ed1ab_0 --> 3.1.1-pyhd8ed1ab_0\n  pluggy             conda-forge/linux-64::pluggy-1.0.0-py~ --> conda-forge/noarch::pluggy-1.0.0-pyhd8ed1ab_5\n  pyzmq                               25.0.0-py38he24dcef_0 --> 25.0.1-py38he24dcef_0\n  qtawesome                              1.2.2-pyhd8ed1ab_0 --> 1.2.3-pyhd8ed1ab_0\n  qtconsole                              5.4.0-pyhd8ed1ab_0 --> 5.4.1-pyhd8ed1ab_0\n  qtconsole-base                         5.4.0-pyha770c72_0 --> 5.4.1-pyha770c72_0\n  requests-futures   conda-forge/linux-64::requests-future~ --> conda-forge/noarch::requests-futures-1.0.0-pyh9f0ad1d_2\n  simplejson                          3.18.3-py38h1de0b5d_0 --> 3.18.4-py38h1de0b5d_0\n  snappy                                   1.1.9-hbd366e4_2 --> 1.1.10-h9fff704_0\n  thrift_sasl        conda-forge/linux-64::thrift_sasl-0.4~ --> conda-forge/noarch::thrift_sasl-0.4.3-pyhd8ed1ab_2\n  typing-extensions                        4.4.0-hd8ed1ab_0 --> 4.5.0-hd8ed1ab_0\n  typing_extensions                      4.4.0-pyha770c72_0 --> 4.5.0-pyha770c72_0\n  wheel                                 0.38.4-pyhd8ed1ab_0 --> 0.40.0-pyhd8ed1ab_0\n\nThe following packages will be SUPERSEDED by a higher-priority channel:\n\n  ipython            conda-forge/noarch::ipython-8.11.0-py~ --> conda-forge/linux-64::ipython-8.4.0-py38h578d9bd_0\n  terminado          conda-forge/noarch::terminado-0.17.1-~ --> conda-forge/linux-64::terminado-0.15.0-py38h578d9bd_0\n\nThe following packages will be DOWNGRADED:\n\n  pyqt                                5.12.3-py38h578d9bd_8 --> 5.12.3-py38ha8c2ead_4\n\n\n\nDownloading and Extracting Packages\npandoc-3.1.1         | 26.2 MB   | ##################################### | 100% \nsnappy-1.1.10        | 38 KB     | ##################################### | 100% \nthrift_sasl-0.4.3    | 13 KB     | ##################################### | 100% \nsimplejson-3.18.4    | 104 KB    | ##################################### | 100% \ncoverage-7.2.2       | 270 KB    | ##################################### | 100% \nbokeh-3.1.0          | 5.3 MB    | ##################################### | 100% \njupyter_latex_envs-1 | 735 KB    | ##################################### | 100% \nparquet-cpp-1.5.1    | 3 KB      | ##################################### | 100% \nterminado-0.15.0     | 28 KB     | ##################################### | 100% \nqtawesome-1.2.3      | 1.4 MB    | ##################################### | 100% \npluggy-1.0.0         | 16 KB     | ##################################### | 100% \nboto3-1.26.93        | 76 KB     | ##################################### | 100% \nwheel-0.40.0         | 54 KB     | ##################################### | 100% \ntyping-extensions-4. | 9 KB      | ##################################### | 100% \npyqt-5.12.3          | 6.5 MB    | ##################################### | 100% \npickleshare-0.7.5    | 9 KB      | ##################################### | 100% \nqtconsole-5.4.1      | 7 KB      | ##################################### | 100% \nalsa-lib-1.2.7.2     | 581 KB    | ##################################### | 100% \npathspec-0.11.1      | 37 KB     | ##################################### | 100% \npyzmq-25.0.1         | 440 KB    | ##################################### | 100% \npexpect-4.8.0        | 48 KB     | ##################################### | 100% \ngst-plugins-base-1.2 | 2.8 MB    | ##################################### | 100% \nnss-3.89             | 1.9 MB    | ##################################### | 100% \nbotocore-1.29.93     | 5.6 MB    | ##################################### | 100% \nrequests-futures-1.0 | 10 KB     | ##################################### | 100% \ntyping_extensions-4. | 31 KB     | ##################################### | 100% \nipython-8.4.0        | 1.1 MB    | ##################################### | 100% \nexceptiongroup-1.1.1 | 18 KB     | ##################################### | 100% \nqtconsole-base-5.4.1 | 95 KB     | ##################################### | 100% \nplatformdirs-3.1.1   | 17 KB     | ##################################### | 100% \nfilelock-3.10.0      | 14 KB     | ##################################### | 100% \ngiflib-5.2.1         | 76 KB     | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: / + /opt/conda/miniconda3/bin/python -c 'import logging; from jupyter_contrib_core.notebook_compat.nbextensions import uninstall_nbextension_python; uninstall_nbextension_python('\\''latex_envs'\\'', sys_prefix=True, logger=logging.getLogger())'\n\n- \ndone\n\nNote: you may need to restart the kernel to use updated packages.\n"}], "source": "conda update jupyterlab"}, {"cell_type": "markdown", "id": "821a2d3b-0c12-4ebc-a7db-91ad51fefd29", "metadata": {}, "source": "## Introduction\n\n[importance/potential effect of voter ID law]\n\nThirty-five of the fifty states of the U.S. have passed stricter voter ID laws that require or request voters to present a form of identification at the polls. \nThe remaining fifteen states do not require voters to present any documentation to vote at the polls. States such as Indiana, Wisconsin, and Tennessee have strict photo ID laws for voters, while states such as Minnesota, Nebraska, North Carolina, and Pennsylvania have no requirements for voter identification. A visualization of the levels of strictness of voter photo identification laws for each state can be seen in the graphic below.\n\n![Voter ID Laws](GCS/voteridmap.png)\n\nAdvantages of implementing stricter voter identification requirements include preventing voter impersonation, thus  increasing public confidence in election processes. Disadvantages of implementing stricter laws unnecessarily burdens voters and administrators.\n\n## Goals\nIn this project we focus our investigations of voter identification laws on the state of Indiana, which implemented a strict voter identification law in 2008. We seek to analyze how much voter turnout would have decreased or increased without the implentation of the law. \n\n> Project Goals\n> - Apply the matching method on pre- voter identification law features.\n> - Conduct k-Nearest-Neighbors (k-NN) classification to make predictions on voter data and cross validation to evaluate.\n> - Strengthen our pyspark data analysis skills, collaborative skills, and project organization skills\n\n[technologies, packages, skills...]"}, {"cell_type": "markdown", "id": "52ff4d48-f0d5-44ff-ad6c-638ef44b5a47", "metadata": {}, "source": "## Indiana Voter Data\n\n### Dataset Overview\n\nOur data is from the course's voter files folder. We primarily use the dataset corresponding to Indiana. At a glance, the dataset contains 726 columns and 946908 rows, records beginning from .... and ending at March 5, 2021\n\n[eda/visualizations]\n\n### Data Cleaning\n\nMany of the columns of the dataset have missing values.\nWe narrowed down our focus to individuals who were of the legal voting age of 18 or older at the time of voting.\n\n\nWe subsetted the dataset to focus on a narrower set of voter attributes. We selected the following columns from the original dataset:\n\n[table with column names and descriptions]\n\n\n"}, {"cell_type": "code", "execution_count": 1, "id": "450b2d06-21f9-4956-9afb-3a953ee7ccb3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/03/17 20:56:52 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n23/03/17 20:56:52 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n23/03/17 20:56:52 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n23/03/17 20:56:52 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"}, {"name": "stdout", "output_type": "stream", "text": "Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.3\n      /_/\n\nUsing Python version 3.8.15 (default, Nov 22 2022 08:46:39)\nSpark context Web UI available at http://mycluster-m.c.pstat135-235.internal:39547\nSpark context available as 'sc' (master = yarn, app id = application_1679079816687_0001).\nSparkSession available as 'spark'.\n"}], "source": "{\n    \"tags\": [\n        \"remove-cell\"\n    ]\n}\n\n# Importing necessary modules\nimport seaborn as sns\nimport pyspark.sql.functions as F\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom operator import add\nfrom functools import reduce\nimport numpy as np\nimport re\nimport os\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\nfrom pyspark.sql.functions import *\nimport random\nimport pyspark\nfrom pyspark.shell import spark\n\nfrom pyspark.sql import SparkSession\n\n\n# Setting up visualization\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline"}, {"cell_type": "code", "execution_count": 5, "id": "5e718f24-fe9a-4fbf-9c77-e857aa041696", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/03/17 05:33:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:34:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:34:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:34:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:34:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:35:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:35:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:35:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:35:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:36:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:36:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:36:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n[Stage 0:>                                                          (0 + 0) / 1]\r"}, {"ename": "KeyboardInterrupt", "evalue": "", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m indi_full \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://voter-project-235-25/VM2Uniform--IN--2021-01-15_parq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:458\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    453\u001b[0m recursiveFileLookup \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursiveFileLookup\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema, pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m    455\u001b[0m                recursiveFileLookup\u001b[38;5;241m=\u001b[39mrecursiveFileLookup, modifiedBefore\u001b[38;5;241m=\u001b[39mmodifiedBefore,\n\u001b[1;32m    456\u001b[0m                modifiedAfter\u001b[38;5;241m=\u001b[39mmodifiedAfter)\n\u001b[0;32m--> 458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n", "\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}, {"name": "stderr", "output_type": "stream", "text": "23/03/17 05:36:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:37:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:37:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:37:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:37:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:38:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:38:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:38:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:38:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:39:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:39:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:39:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:39:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:40:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:40:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:40:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:40:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:41:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:41:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:41:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:41:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:42:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:42:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:42:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:42:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:43:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:43:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:43:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:43:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:44:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:44:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:44:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:44:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:45:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:45:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:45:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:45:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:46:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:46:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:46:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:46:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:47:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:47:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:47:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:47:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:48:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:48:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:48:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:48:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:49:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:49:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:49:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:49:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:50:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:50:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:50:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:50:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:51:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:51:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:51:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:51:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:52:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:52:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:52:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:52:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:53:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:53:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:53:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:53:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:54:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:54:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:54:36 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:54:51 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:55:06 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/03/17 05:55:21 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"}], "source": "indi_full = spark.read.parquet(\"gs://voter-project-235-25/VM2Uniform--IN--2021-01-15_parq\")"}, {"cell_type": "code", "execution_count": null, "id": "8c50878b-4d74-4144-b57b-869ef5ccd83e", "metadata": {}, "outputs": [], "source": "cols_to_keep = [\n    \"Voters_Gender\",\n    \"Voters_Age\",\n    \"Voters_BirthDate\",\n    \"Residence_Families_HHCount\",\n    \"Residence_HHGender_Description\",\n    \"Mailing_Families_HHCount\",\n    \"Mailing_HHGender_Description\",\n\n#   !! voter party affiliation\n    \"Parties_Description\", \n    \n    \"CommercialData_PropertyType\",\n    \"AddressDistricts_Change_Changed_CD\",\n    \"AddressDistricts_Change_Changed_SD\",\n    \"AddressDistricts_Change_Changed_HD\",\n    \"AddressDistricts_Change_Changed_County\",\n    \"Residence_Addresses_Density\",\n    \"CommercialData_EstimatedHHIncome\",\n    \"CommercialData_ISPSA\",\n    \"CommercialData_AreaMedianEducationYears\",\n    \"CommercialData_AreaMedianHousingValue\",\n    \"CommercialData_MosaicZ4Global\",\n    \"CommercialData_AreaPcntHHMarriedCoupleNoChild\",\n    \"CommercialData_AreaPcntHHMarriedCoupleWithChild\",\n    \"CommercialData_AreaPcntHHSpanishSpeaking\",\n    \"CommercialData_AreaPcntHHWithChildren\",\n    \"CommercialData_StateIncomeDecile\",\n    \"Ethnic_Description\",\n    \"EthnicGroups_EthnicGroup1Desc\",\n    \"CommercialData_DwellingType\",\n    \"CommercialData_PresenceOfChildrenCode\",\n    \"CommercialData_PresenceOfPremCredCrdInHome\",\n    \"CommercialData_DonatesToCharityInHome\",\n    \"CommercialData_DwellingUnitSize\",\n    \"CommercialData_ComputerOwnerInHome\",\n    \"CommercialData_DonatesEnvironmentCauseInHome\",\n    \"CommercialData_Education\",\n    \"General_2000\",\n    \"General_2004\",\n    \"PresidentialPrimary_2000\",\n    \"PresidentialPrimary_2004\",\n        \n#   Outcome variable (indiana law happens in 2005, approved by SCOTUS before presidential election in 2008)\n    \"General_2008\"\n]\n\nindi = (indi_full\n        .select(cols_to_keep))"}, {"cell_type": "markdown", "id": "562385eb-ce20-4531-a9b4-136523d766c1", "metadata": {}, "source": "Based on the voter's age, we calculate the date at which they turn eighteen. We create a new variable whose value is the year of the earliest election that the voter could potentially participate in. So, if the date at which they turn eighteen is earlier than November 3rd, we set the value to the year at which they turn eighteen. If the date at which they turn eighteen is later than November 3rd, we set the value to the year of the following election."}, {"cell_type": "code", "execution_count": null, "id": "e6b98dee-96c2-402b-b2c4-c569ae88d5e7", "metadata": {}, "outputs": [], "source": "yrs_add = 18\nmonths_add = 18*12\n\n# date of national \ntarget_month_day_presidential = \"11-03\"\n\n# date of Indiana's presidential primary\ntarget_month_day_primary = \"05-03\" \n\nindi = indi.withColumn(\"DATE_18\", add_months(to_date(col(\"Voters_BirthDate\"),\"MM/dd/yyyy\"), months_add))\nindi.select([\"Voters_BirthDate\", \"DATE_18\"]).show(10)\nindi = indi.dropna(subset = \"Voters_BirthDate\")\nindi = indi.withColumn(\"YEAR_18\", year(\"DATE_18\"))\nindi = indi.withColumn(\"comparator_date_presidential\", to_date(concat(col(\"YEAR_18\"), lit(\"-\"), lit(target_month_day_presidential))))\nindi = indi.withColumn(\"comparator_date_primary\", to_date(concat(col(\"YEAR_18\"), lit(\"-\"), lit(target_month_day_primary))))\nindi = indi.withColumn(\"YEAR_ELIGIBLE_TO_VOTE_PRESIDENTIAL\", \\\n                             when(col(\"DATE_18\")<=col(\"comparator_date_presidential\"), col(\"YEAR_18\")) \\\n                             .otherwise(col(\"YEAR_18\") + 1) \\\n                            )\nindi = indi.withColumn(\"YEAR_ELIGIBLE_TO_VOTE_PRIMARY\", \\\n                             when(col(\"DATE_18\")<=col(\"comparator_date_primary\"), col(\"YEAR_18\")) \\\n                             .otherwise(col(\"YEAR_18\") + 1) \\\n                            )\n\n# check no missing vals:\nindi.where(col(\"YEAR_18\").isNull()).select(\"YEAR_18\").show(10)\n\n# get rid of rows where the voter was not old enough to vote in the 2008 general election\nindi = indi.filter(col(\"YEAR_ELIGIBLE_TO_VOTE_PRESIDENTIAL\")<=2008).fillna(\"N\", subset = [\"General_2008\"])\n\n# for the 2000 and 2004 general elections, replace with \"N\" IF the person was old enough to vote at the time\nindi = indi.withColumn(\"General_2000\", \\\n                      when((col(\"YEAR_ELIGIBLE_TO_VOTE_PRESIDENTIAL\")<= 2004) & \\\n                           (col(\"General_2000\").isNull()), \"N\") \\\n                      .otherwise(col(\"General_2000\")) \\\n                      )\n\nindi = indi.withColumn(\"General_2004\", \\\n                      when((col(\"YEAR_ELIGIBLE_TO_VOTE_PRESIDENTIAL\")<= 2004) & \\\n                           (col(\"General_2004\").isNull()), \"N\") \\\n                      .otherwise(col(\"General_2004\")) \\\n                      )\n\n# do the same for the primaries:\nindi = indi.withColumn(\"PresidentialPrimary_2000\", \\\n                      when((col(\"YEAR_ELIGIBLE_TO_VOTE_PRIMARY\")<= 2004) & \\\n                           (col(\"PresidentialPrimary_2000\").isNull()), \"N\") \\\n                      .otherwise(col(\"PresidentialPrimary_2000\")) \\\n                      )\n\nindi = indi.withColumn(\"PresidentialPrimary_2004\", \\\n                      when((col(\"YEAR_ELIGIBLE_TO_VOTE_PRIMARY\")<= 2004) & \\\n                           (col(\"PresidentialPrimary_2004\").isNull()), \"N\") \\\n                      .otherwise(col(\"PresidentialPrimary_2004\")) \\\n                      )\n\n# make the general voting for 2008 a numeric variable; since we've deleted\n# everyone who was not eligible to vote, this can be directly calculated with a 1-0.\nindi = indi.withColumn(\"Voted_General_2008\", when(indi.General_2008 == \"Y\",1).otherwise(0))\nindi = indi.drop(\"General_2008\")"}, {"cell_type": "markdown", "id": "9c70f7d0-10ad-4dca-885f-aa3ef77395ce", "metadata": {}, "source": "We begin by obtaining a subset of the dataset to prototype code.\n"}, {"cell_type": "code", "execution_count": null, "id": "074fd787-26dd-4006-a93b-109bc7ef1261", "metadata": {}, "outputs": [], "source": "sampleind = indi.sample(True, 0.1, seed = 19480384)"}, {"cell_type": "markdown", "id": "b311b115-3859-4337-8c56-73cacfcf424d", "metadata": {}, "source": "We then convert the column `CommercialData_EstimatedHHIncome` from type string to type numeric by removing the right-most number, and replacing all symbols \"$\", \"-\", and \"+\"."}, {"cell_type": "code", "execution_count": null, "id": "db41cf37-8feb-4f66-aaf2-a8b1bdaa2c54", "metadata": {}, "outputs": [], "source": "sampleind = sampleind.withColumn(\"CommercialData_EstimatedHHIncome\", regexp_extract(col(\"CommercialData_EstimatedHHIncome\"), \"(?<=-).*\", 0))\n\nsampleind = sampleind.withColumn(\"CommercialData_EstimatedHHIncome\", \\\n                             regexp_replace('CommercialData_EstimatedHHIncome', \"[\\$,+]\", \"\") \\\n                            )\n\nsampleind = sampleind.withColumn(\"CommercialData_EstimatedHHIncome\",col(\"CommercialData_EstimatedHHIncome\").cast('double'))\n\nsampleind.select([\"CommercialData_EstimatedHHIncome\"]).show(10, truncate=False)\n"}, {"cell_type": "markdown", "id": "bb305a3b-c61f-4042-93ee-8fb4e8682d15", "metadata": {}, "source": "We also convert the column `CommercialData_AreaMedianHousingValue` from type string to type numeric by replacing the symbol \"$\"."}, {"cell_type": "code", "execution_count": null, "id": "a51be547-5be1-45d7-afc3-debae785e7a4", "metadata": {}, "outputs": [], "source": "sampleind = sampleind.withColumn(\"CommercialData_AreaMedianHousingValue\", regexp_replace(\"CommercialData_AreaMedianHousingValue\", \"\\$\", \"\"))\nsampleind = sampleind.withColumn(\"CommercialData_AreaMedianHousingValue\",col(\"CommercialData_AreaMedianHousingValue\").cast('double'))\nsampleind.select([\"CommercialData_AreaMedianHousingValue\"]).show(10, truncate=False)"}, {"cell_type": "markdown", "id": "a29db863-dd0d-4a57-a48d-304e6ca420ab", "metadata": {}, "source": "We proceed to search for the string \"Pnct\" in all of the column names in our dataset, and convert these columns\n\n> - 'CommercialData_AreaPcntHHMarriedCoupleNoChild'\n> - 'CommercialData_AreaPcntHHMarriedCoupleWithChild'\n> - 'CommercialData_AreaPcntHHSpanishSpeaking'\n> -'CommercialData_AreaPcntHHWithChildren'\n \nto numeric types by replacing the symbol \"%\".\n"}, {"cell_type": "code", "execution_count": null, "id": "5ca1254c-3dd3-4cdb-9d83-16af0219e08b", "metadata": {}, "outputs": [], "source": "cols_to_convert = [c for c in sampleind.columns if \"Pcnt\" in c]\n\nfor col_name in cols_to_convert:\n    sampleind = sampleind.withColumn(col_name, regexp_replace(col_name, \"\\%\", \"\"))\n    sampleind = sampleind.withColumn(col_name, col(col_name).cast('double'))\n    sampleind.select([col_name]).show(5, truncate=False)\n    "}, {"cell_type": "markdown", "id": "b570a674-34cd-47f9-8fff-22027a86df4d", "metadata": {}, "source": "We then remove the columns that were used for obtaining voter turnout data from our dataset.\n\n"}, {"cell_type": "code", "execution_count": null, "id": "b8a14c2f-3e0f-491a-8d7c-5450500e9527", "metadata": {}, "outputs": [], "source": "columns_to_drop = [\"comparator_date_presidential\", \"target_month_day_primary\", \n                   \"YEAR_ELIGIBLE_TO_VOTE_PRESIDENTIAL\", \"comparator_date_primary\", \n                   \"YEAR_ELIGIBLE_TO_VOTE_PRIMARY\", \"YEAR_18\", \"DATE_18\"]\n\nsampleind = sampleind.drop(*columns_to_drop)"}, {"cell_type": "markdown", "id": "51ba59e8-3537-4c35-aee3-5b7a9eee6d88", "metadata": {"tags": []}, "source": "## Regression\nRegression is parametric as it has a function linking the treatment $D$ and covariates $X$ with the outcome $Y$, and has parameters $\\beta$'s to be estimated\n\n\n### Issues\n- doesn\u2019t estimate weight based on weighted average- proportional on variance of the treatment in that group\n- Assumes linear relationships bt covariates and outcome\n- Underfitting, underestimating\n- instead: knn for matching thing\n\n## Application to Voter Data\nVariables:\n> - x: (sex)\n> - T=treatment assignment under natural circumstances(1 if you have a voter identification law passed in your state; 0 otherwise) (drug)\n> - Y= treatment effect/outcome (voter turnout) (days)\n> - Y1,Y0= realized outcomes (if you were treated (your state has the voter identification law), if you were not treated (your state has no voter identification law))\n> - Average treatment effect (how much voter turnout would have increased or decreased without the implementation of the voter identification law)\n> - If we have independence between (Y0,Y1) and T, easy\n> - If not, must do things including matching\n\nWe will follow the matching process for voter data from Indiana, then repeat it for another state [state] for comparison.\n\n\n"}, {"cell_type": "markdown", "id": "11734024-a001-421e-85de-a420cf7a5b1e", "metadata": {}, "source": "## Propensity Score\nidea: wanna predict if someone has passed the law or not\nFind probability if they vote if they did not have the voter identification law\n\nAssumption: ppl outside Indiana are representative of ppl in indiana\n\n\n> Variables:\n> - T = whether they have the law\n> - Y = whether they voted in 2008\n> - P = predicted T\n\nTo compare the voter data with and without the implementation of a voter identification law, we observe that the difference in means $E[Y|T=1]-E[Y|T=0]$ is [...]. Thus, the treated voters (with implementation of a voter identification law) have a [...] compared to non-treated voters.\n\nThe propensity score is the conditional probability of receiving the treatment, the implementation of the voter identification law. Using this score means that we do not have to achieve conditional independence $(Y_1,Y_0) \\perp |X$. In other words, we do not have to condition on the whole $X$ to achieve independence of potential outcomes of the treatment. Instead, it is sufficient to control confounders $X$ for a propensity score $P(x)=P(T|X)=E[T|X]$ to achieve $(Y_1,Y_0) \\perp |P(x)$.\n\nThe propensity score essentially converts $X$ into the treatment $T$, acting as a middleman between $X$ and $T$. Initially, we cannot compare treated and non-treated osbervations. However, we can compare a treated and a non-treated observation if they have the same probability of receiving the treatment since receiving or not receiving the treatment would be attributed to randomness. Thus, we hold the propensity score constant to make the data appear more random.\n\n\n### Estimation\n\nWe now estimate the true propensity score $P(x)$ with $\\hat{P}(x)$ using logistic regression.\n\nIn order to complete this, we convert categorical variables in our dataset into dummy variables.\n\n\n> Issues\n> - The propensity score's strong predictive power can hurt our goals of causal inference.\n>> - We want out prediction to control for confounding variables, not neccesarily to predict the treatment very well."}, {"cell_type": "markdown", "id": "9abc2e49-f80b-4ba0-85a2-1b2a0a42e8bc", "metadata": {}, "source": "## Logistic Regression\n\nLogistic regression is a statistical method\n- use non-indiana data to predict Indiana data"}, {"cell_type": "markdown", "id": "64a2c481-e077-4f9d-8692-88cf9d0c097d", "metadata": {}, "source": "## Summary of Findings\n\n\n## Conclusion\n\n[summary of everything]\n\n[issues - curse of dimensionality]\n\n[significance]\n\n[possible future work]"}, {"cell_type": "markdown", "id": "2ac20cc5-b91b-455b-be9e-11990075871c", "metadata": {}, "source": "## Resources\nhttps://www.ncsl.org/elections-and-campaigns/voter-id#undefined \n\nhttps://www.franciscoyira.com/post/matching-in-r-2-differences-regression/\n"}, {"cell_type": "code", "execution_count": null, "id": "63204e4b-8a00-47c5-992b-70d8c4cf06a8", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}