<<<<<<< HEAD
{"cells": [{"cell_type": "markdown", "id": "a5b28054-4af3-4738-b79c-dd228e99c35f", "metadata": {}, "source": "# Analysis of Voter Turnout in Indiana Pre- and Post- Voter Identification Law\n### Authors: Christopher Lefrak, Hannah Li, George Yang, and Kuai Yu\n### PSTAT 235\n\nNOTES/TO-DO (delete this before submitting):\n- truncate/limit code so the writeup looks polished and professional (no raw outputs/errors)\n- interpret findings\n- include visualizations and graphs (EDA? theoretical concepts?)\n- run and write ONLY NECESSARY code (label all code clearly and eliminate commented out code) in the \"Final Code file.ipynb\" file, save all visualizations as images to embedd in this writeup document so it's more organized.\n- submission: \"Writeup.ipynb\" file and \"Final Code file.ipynb\"\n"}, {"cell_type": "markdown", "id": "821a2d3b-0c12-4ebc-a7db-91ad51fefd29", "metadata": {}, "source": "## Introduction\n\n[importance/potential effect of voter ID law]\n\nThirty-five of the fifty states of the U.S. have passed stricter voter ID laws that require or request voters to present a form of identification at the polls. \nThe remaining fifteen states do not require voters to present any documentation to vote at the polls. States such as Indiana, Wisconsin, and Tennessee have strict photo ID laws for voters, while states such as Minnesota, Nebraska, North Carolina, and Pennsylvania have no requirements for voter identification. A visualization of the levels of strictness of voter photo identification laws for each state can be seen in the map in **Fig. 1** below.\n\n<figure>\n<img src=\"https://drive.google.com/uc?export=view&id=1-bKVXaRl_j3trfCus6iWYylx3RX4ACPz\" style=\"width:100%\">\n<figcaption align = \"center\"> Fig. 1: Strictness Levels of US Voter ID Laws </figcaption>\n</figure>\n\nAdvantages of implementing stricter voter identification requirements include preventing voter impersonation, thus  increasing public confidence in election processes. Disadvantages of implementing stricter laws unnecessarily burdens voters and administrators.\n\n## Goals\nIn this project we focus our investigations of voter identification laws on the state of Indiana, which implemented a strict voter identification law in 2008. We seek to analyze how much voter turnout would have decreased or increased without the implentation of the law. \n\n> Project Goals\n> - Propensity score \n> - Conduct logistic regression\n> - Strengthen our PySpark data analysis skills, collaborative skills, and project organization skills"}, {"cell_type": "markdown", "id": "52ff4d48-f0d5-44ff-ad6c-638ef44b5a47", "metadata": {}, "source": "## Voter Data\n\n### Dataset Overview\n\nOur voter data is obtained from the course's VM2Uniform folder. We primarily use the dataset corresponding to Indiana. At a glance, the dataset contains 726 columns and 946908 rows, records beginning from .... and ending in 2021\n\n[eda/visualizations]\n\nWe subsetted the dataset to focus on a narrower set of voter attributes. The columns we selected from the original dataset can be seen in the following section.\n\n\n"}, {"cell_type": "markdown", "id": "706cd871-ea95-426f-9a37-369f076189f3", "metadata": {}, "source": "### Input Variables\nIn the table below we display the columns we keep from the original dataset\n\n| Column Name | Data Type | Values |\n| --- | --- | --- |\n| Voters_Gender | categorical | ... |\n| Voters_BirthDate | --- | ... |\n| Residence_Families_HHCount | numerical | 1 through 10 |\n| Residence_HHGender_Description | categorical | Cannot Determine, Female Only Household, Male Only Household, Mixed Gender Household |\n| Mailing_Families_HHCount | numerical |  ... |\n| Mailing_HHGender_Description | categorical |  ... |\n| Parties_Description | categorical | ... |\n| CommercialData_PropertyType | categorical | ... |\n| AddressDistricts_Change_Changed_CD | categorical | ... |\n| AddressDistricts_Change_Changed_SD | categorical | ... |\n| AddressDistricts_Change_Changed_HD | categorical | ... |\n| AddressDistricts_Change_Changed_County | categorical | ... |\n| Residence_Addresses_Density | numerical | ... |\n| CommercialData_EstimatedHHIncome | categorical | ... |\n| CommercialData_ISPSA | categorical | 0 through 9 |\n| CommercialData_AreaMedianEducationYears | numerical |  ... |\n| CommercialData_AreaMedianHousingValue | numerical | ... |\n| CommercialData_AreaPcntHHMarriedCoupleNoChild | categorical | ... |\n| CommercialData_AreaPcntHHMarriedCoupleWithChild | categorical | ... |\n| CommercialData_AreaPcntHHSpanishSpeaking | categorical | ... |\n| CommercialData_AreaPcntHHWithChildren | categorical | ... |\n| CommercialData_StateIncomeDecile | categorical | 0 through 9 |\n| EthnicGroups_EthnicGroup1Desc | categorical |  East and South Asian, Eurpoean, Hispanic and Portuguese, Likely African-American, N/A, Other |\n| CommercialData_DwellingType | categorical | Large mult wo/Apt number, POBOX, Single Family Dwelling Unit, Small Mult or large mult w/apt number |\n| CommercialData_PresenceOfChildrenCode | categorical | Known Data, Modeled Likely to have a child, Modeled Not as Likely to have a child, Not Likely to have a child |\n| CommercialData_DonatesToCharityInHome | categorical | Yes, unknown |\n| CommercialData_DwellingUnitSize | categorical | Single Family Dwelling Unit, Duplex, Triplex, 4, 5-9, 10-19, 20-49, 50-100, 101+ |\n| CommercialData_ComputerOwnerInHome | categorical | Yes, unknown |\n| CommercialData_DonatesEnvironmentCauseInHome | categorical | Yes, unknown |\n| CommercialData_Education | categorical | Unknown, HS Diploma - Extremely Likely, Some College -Extremely Likely, Bach Degree - Extremely Likely, Grad Degree - Extremely Likely, Less than HS Diploma - Ex Like, HS Diploma - Likely, Some College - Likely, Bach Degree - Likely, Grad Degree - Likely, Less than HS Diploma - Likely |\n\n\n### Other Variables\nThe table below shows other control variables that we expect to be highly associated with the response variable.\n\n| Column Name | Data Type | Values |\n| --- | --- | --- |\n| General_2000 | categorical |  |\n| General_2004 | categorical |  |\n| PresidentialPrimary_2000 | categorical | --- |\n| PresidentialPrimary_2004 | categorical |  |\n| General_2008 | categorical | --- |\n\nThe table below shows the response variable.\n\n| Column Name | Data Type | Values |\n| --- | --- | --- |\n| General_2008 | categorical |  |"}, {"cell_type": "markdown", "id": "50b2c95a-63d7-4dfc-9ced-29d69e89abb0", "metadata": {}, "source": "### Other States\nIn the table below are states that do not have strict voter identification laws.\n\n| State | Abbreviation |\n| --- | --- |\n| California | CA | \n| Illinois | IL |\n| Massachusetts | MA | \n| Maryland | MD | \n| Maine | ME |\n| Minnesota | MN | \n| North Carolina | NC | \n| Nebraska | NE |\n| New Jersey | NJ | \n| New Mexico | NM | \n| Nevada | NV |\n| New York | NY | \n| Oregon | OR | \n| Pennsylvania | PA |\n| Vermont | VT | \n"}, {"cell_type": "markdown", "id": "562385eb-ce20-4531-a9b4-136523d766c1", "metadata": {}, "source": "### Data Cleaning\n\nMany of the columns contain symbols including `$` and `%`, so we remove those symbols.\n\nMany columns are also missing data. In numerical columns, we impute these values in with the mean value to minimize any changes to z-scores of the given data. We encode categorical columns using PySpark's `StringIndexer`. This maps the categorical labels of a column a column of label indices ordered by frequencies of labels, where the most frequent label is assigned to index 0.\n\n\nOur data also contains many individuals who were not old enough to vote in the 2008 general election (they were below the 18 year old age requirement), so we removed the rows corresponding to these voters. \nWe then converted the `General_2008` variable to be numerical. \n"}, {"cell_type": "markdown", "id": "11734024-a001-421e-85de-a420cf7a5b1e", "metadata": {}, "source": "## Propensity Score\nOur goal is to predict if a voter has passed the law or not.\n\nWe want to be able to find the probability if someone votes if they did not have the voter identification law implented.\n\nAssumption: People outside of Indiana are representative of the people in indiana.\n\n\n> Variables:\n> - T = whether they have the law\n> - Y = whether they voted in 2008\n> - P = predicted T\n\nTo compare the voter data with and without the implementation of a voter identification law, we observe that the difference in means $$E[Y|T=1]-E[Y|T=0]$$ is [...]. Thus, the treated voters (with implementation of a voter identification law) have a [...] compared to non-treated voters.\n\nThe propensity score is the conditional probability of receiving the treatment, the implementation of the voter identification law. Using this score means that we do not have to achieve conditional independence $(Y_1,Y_0) \\perp |X$. In other words, we do not have to condition on the whole $X$ to achieve independence of potential outcomes of the treatment. Instead, it is sufficient to control confounders $X$ for a propensity score $$P(x)=P(T|X)=E[T|X]$$ to achieve $(Y_1,Y_0) \\perp |P(x)$.\n\nThe propensity score essentially converts $X$ into the treatment $T$, acting as a middleman between $X$ and $T$. Initially, we cannot compare treated and non-treated osbervations. However, we can compare a treated and a non-treated observation if they have the same probability of receiving the treatment since receiving or not receiving the treatment would be attributed to randomness. Thus, we hold the propensity score constant to make the data appear more random.\n\n\n### Propensity Weighting and Estimation\n\nWe write the difference in means again, but we now condition on $X$: \n\n$$\n\\begin{align}\n& E[Y|X,T=1]-E[Y|X,T=0]\\\\\n& =E[\\frac{Y}{P(x)}|X,T=1]P(x)-E[\\frac{Y}{(1-P(x))}|X,T=0](1-P(x))\n\\end{align}\n$$\n\nIn other words, the propensity score more heavily weights the observations with a low probability of receiving treatment, and weakly weights...\n\nWe simplify our propensity score weighting estimator to $E[Y\\frac{T-P(x)}{P(x)(1-P(x)}]$, where $P(x)$ and $(1-P(x))$ both must be greater than $0$. Thus, each voter must have some probability of both receiving and not receiving the treatment of the implementation of a voter identification law.\n\n\nWe now estimate the true propensity score $P(x)$ with $\\hat{P}(x)$ using logistic regression.\n\n> Issues\n> - The propensity score's strong predictive power can hurt our goals of causal inference.\n>> - We want out prediction to control for confounding variables, not neccesarily to predict the treatment very well."}, {"cell_type": "markdown", "id": "9abc2e49-f80b-4ba0-85a2-1b2a0a42e8bc", "metadata": {}, "source": "## Logistic Regression\n\nLogistic regression is a statistical method that allows us to estimate the probability that an event occurs, in this case, if an individual voted or not, given a set of independent variables $X_1, X_2,...X_k$. The function can be expressed in terms of the log odds, where the odds $\\frac{x}{1-x}$ are defined as the probability of success divided by the probability of failure:\n\n$$\n\\begin{align}\nlogit(x)&=\\frac{1}{1+e^{-x}} \\\\\nln(\\frac{x}{1-x})&=\\beta_0+\\beta_1 X_1+\\beta_2 X_2+...\\beta_k X_k\n\\end{align}\n$$\n\nThe coefficients of this function are $\\beta_0, \\beta_1, \\beta_2, \\beta_k$, and indicate the relative effect of the corresponding variables $X_1,X_2,...X_K$ on the response variable. The optimal coefficients maximize the function to find the best fit.\n\n\nApplying this to our data, we split our dataset into 70% training data and 30% testing data, and fit logistic regression on our training data. \n\nA plot of the beta coefficients can be seen in **Fig. 2** and a plot of the ranked coefficients can be seen in **Fig. 3**.\n\n<figure>\n<img src=\"...\" style=\"width:100%\">\n<figcaption align = \"center\"> Fig. 2: Logistic Regression Beta Coefficients </figcaption>\n</figure>\n\n\n<figure>\n<img src=\"...\" style=\"width:100%\">\n<figcaption align = \"center\"> Fig. 3: Logistic Regression Ranked Coefficients </figcaption>\n</figure>\n\nThe propensity score we obtain is [...]\n"}, {"cell_type": "markdown", "id": "64a2c481-e077-4f9d-8692-88cf9d0c097d", "metadata": {}, "source": "## Summary of Findings\n\n\n## Conclusion\n\n[summary of everything]\n\n[issues - curse of dimensionality]\n\n[significance]\n\n[possible future work]"}, {"cell_type": "markdown", "id": "2ac20cc5-b91b-455b-be9e-11990075871c", "metadata": {}, "source": "## Resources\nhttps://www.ncsl.org/elections-and-campaigns/voter-id#undefined \n\nhttps://matheusfacure.github.io/python-causality-handbook/11-Propensity-Score.html\n\nhttps://www.ibm.com/topics/logistic-regression#:~:text=Resources-,What%20is%20logistic%20regression%3F,given%20dataset%20of%20independent%20variables."}, {"cell_type": "code", "execution_count": null, "id": "63204e4b-8a00-47c5-992b-70d8c4cf06a8", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b28054-4af3-4738-b79c-dd228e99c35f",
   "metadata": {},
   "source": [
    "# Analysis of Voter Turnout in Indiana Pre- and Post- Voter Identification Law\n",
    "### Authors: Christopher Lefrak, Hannah Li, George Yang, and Kuai Yu\n",
    "### PSTAT 235\n",
    "\n",
    "NOTES/TO-DO (delete this before submitting):\n",
    "- truncate/limit code so the writeup looks polished and professional (no raw outputs/errors)\n",
    "- interpret findings\n",
    "- include visualizations and graphs (EDA? theoretical concepts?)\n",
    "- run and write ONLY NECESSARY code (label all code clearly and eliminate commented out code) in the \"Final Code file.ipynb\" file, save all visualizations as images to embedd in this writeup document so it's more organized.\n",
    "- submission: \"Writeup.ipynb\" file and \"Final Code file.ipynb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a2d3b-0c12-4ebc-a7db-91ad51fefd29",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[importance/potential effect of voter ID law]\n",
    "\n",
    "Thirty-five of the fifty states of the U.S. have passed stricter voter ID laws that require or request voters to present a form of identification at the polls. \n",
    "The remaining fifteen states do not require voters to present any documentation to vote at the polls. States such as Indiana, Wisconsin, and Tennessee have strict photo ID laws for voters, while states such as Minnesota, Nebraska, North Carolina, and Pennsylvania have no requirements for voter identification. A visualization of the levels of strictness of voter photo identification laws for each state can be seen in the map in **Fig. 1** below.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1-bKVXaRl_j3trfCus6iWYylx3RX4ACPz\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> Fig. 1: Strictness Levels of US Voter ID Laws </figcaption>\n",
    "</figure>\n",
    "\n",
    "Advantages of implementing stricter voter identification requirements include preventing voter impersonation, thus  increasing public confidence in election processes. Disadvantages of implementing stricter laws unnecessarily burdens voters and administrators.\n",
    "\n",
    "## Goals\n",
    "In this project, we conduct a case study to see if implementing laws that inhibit the convenience of voting can ultimately lead to decreased voter participation. We focus our investigations of voter identification laws on the state of Indiana, which implemented a strict voter identification law in 2008. We seek to analyze how much voter turnout would have decreased or increased without the implentation of the law. \n",
    "\n",
    "> Project Goals\n",
    "> - Conduct logistic regression\n",
    "> - Propensity score \n",
    "> - Doubly Robust Estimation\n",
    "> - Strengthen our PySpark data analysis skills, collaborative skills, and project organization skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff4d48-f0d5-44ff-ad6c-638ef44b5a47",
   "metadata": {},
   "source": [
    "## Voter Data\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "Our voter data is obtained from the course's VM2Uniform folder. We primarily use the dataset corresponding to Indiana. At a glance, the dataset contains 726 columns and 946908 rows, records beginning from .... and ending in 2021\n",
    "\n",
    "[eda/visualizations]\n",
    "\n",
    "We subsetted the dataset to focus on a narrower set of voter attributes. The columns we selected from the original dataset can be seen in the following section.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706cd871-ea95-426f-9a37-369f076189f3",
   "metadata": {},
   "source": [
    "### Input Variables\n",
    "In the table below we display the columns we keep from the original dataset\n",
    "\n",
    "| Column Name | Data Type | Values |\n",
    "| --- | --- | --- |\n",
    "| Voters_Gender | categorical | ... |\n",
    "| Voters_BirthDate | --- | ... |\n",
    "| Residence_Families_HHCount | numerical | 1 through 10 |\n",
    "| Residence_HHGender_Description | categorical | Cannot Determine, Female Only Household, Male Only Household, Mixed Gender Household |\n",
    "| Mailing_Families_HHCount | numerical |  ... |\n",
    "| Mailing_HHGender_Description | categorical |  ... |\n",
    "| Parties_Description | categorical | ... |\n",
    "| CommercialData_PropertyType | categorical | ... |\n",
    "| AddressDistricts_Change_Changed_CD | categorical | ... |\n",
    "| AddressDistricts_Change_Changed_SD | categorical | ... |\n",
    "| AddressDistricts_Change_Changed_HD | categorical | ... |\n",
    "| AddressDistricts_Change_Changed_County | categorical | ... |\n",
    "| Residence_Addresses_Density | numerical | ... |\n",
    "| CommercialData_EstimatedHHIncome | categorical | ... |\n",
    "| CommercialData_ISPSA | categorical | 0 through 9 |\n",
    "| CommercialData_AreaMedianEducationYears | numerical |  ... |\n",
    "| CommercialData_AreaMedianHousingValue | numerical | ... |\n",
    "| CommercialData_AreaPcntHHMarriedCoupleNoChild | categorical | ... |\n",
    "| CommercialData_AreaPcntHHMarriedCoupleWithChild | categorical | ... |\n",
    "| CommercialData_AreaPcntHHSpanishSpeaking | categorical | ... |\n",
    "| CommercialData_AreaPcntHHWithChildren | categorical | ... |\n",
    "| CommercialData_StateIncomeDecile | categorical | 0 through 9 |\n",
    "| EthnicGroups_EthnicGroup1Desc | categorical |  East and South Asian, Eurpoean, Hispanic and Portuguese, Likely African-American, N/A, Other |\n",
    "| CommercialData_DwellingType | categorical | Large mult wo/Apt number, POBOX, Single Family Dwelling Unit, Small Mult or large mult w/apt number |\n",
    "| CommercialData_PresenceOfChildrenCode | categorical | Known Data, Modeled Likely to have a child, Modeled Not as Likely to have a child, Not Likely to have a child |\n",
    "| CommercialData_DonatesToCharityInHome | categorical | Yes, unknown |\n",
    "| CommercialData_DwellingUnitSize | categorical | Single Family Dwelling Unit, Duplex, Triplex, 4, 5-9, 10-19, 20-49, 50-100, 101+ |\n",
    "| CommercialData_ComputerOwnerInHome | categorical | Yes, unknown |\n",
    "| CommercialData_DonatesEnvironmentCauseInHome | categorical | Yes, unknown |\n",
    "| CommercialData_Education | categorical | Unknown, HS Diploma - Extremely Likely, Some College -Extremely Likely, Bach Degree - Extremely Likely, Grad Degree - Extremely Likely, Less than HS Diploma - Ex Like, HS Diploma - Likely, Some College - Likely, Bach Degree - Likely, Grad Degree - Likely, Less than HS Diploma - Likely |\n",
    "\n",
    "\n",
    "### Other Variables\n",
    "The table below shows other control variables that we expect to be highly associated with the response variable.\n",
    "\n",
    "| Column Name | Data Type | Values |\n",
    "| --- | --- | --- |\n",
    "| General_2000 | categorical | Y/null |\n",
    "| General_2004 | categorical | Y/null |\n",
    "| PresidentialPrimary_2000 | categorical | Y/null |\n",
    "| PresidentialPrimary_2004 | categorical | Y/null |\n",
    "\n",
    "\n",
    "The table below shows the response variable.\n",
    "\n",
    "| Column Name | Data Type | Values |\n",
    "| --- | --- | --- |\n",
    "| General_2008 | categorical | Y/null |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2c95a-63d7-4dfc-9ced-29d69e89abb0",
   "metadata": {},
   "source": [
    "### Other States\n",
    "In the table below are states that do not have strict voter identification laws.\n",
    "\n",
    "| State | Abbreviation |\n",
    "| --- | --- |\n",
    "| California | CA | \n",
    "| Illinois | IL |\n",
    "| Massachusetts | MA | \n",
    "| Maryland | MD | \n",
    "| Maine | ME |\n",
    "| Minnesota | MN | \n",
    "| North Carolina | NC | \n",
    "| Nebraska | NE |\n",
    "| New Jersey | NJ | \n",
    "| New Mexico | NM | \n",
    "| Nevada | NV |\n",
    "| New York | NY | \n",
    "| Oregon | OR | \n",
    "| Pennsylvania | PA |\n",
    "| Vermont | VT | \n",
    "\n",
    "**Note:** Throughout this report, when we refer to \"other states\", we mean all states from this list above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562385eb-ce20-4531-a9b4-136523d766c1",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Many of the columns contain symbols including `$` and `%`, so we remove those symbols.\n",
    "\n",
    "Many columns are also missing data. In numerical columns, we impute these values in with the mean value to minimize any changes to z-scores of the given data. This is desirable because we are going to standardize our values when implement our machine learning algorithms. We don't want to throw away valuable data and discard the nulls entirely, but we also don't want our imputations to artifically influence/inflate the effect that the predictors may have with the response.\n",
    "\n",
    "Many of the categorical variables in the dataset already have an \"Unknown\" level. As far as we are concerned, if the value is missing, then the value is unknown. Though, this is different for the case that voter participation data may be missing for a given election if the given individual is not old enough to have voted in that election.\n",
    "\n",
    "Our data consists of registered voters as of 2021, so it also contains many individuals who were not old enough to vote in the 2008 general election (they were below the 18 year old age requirement). We removed the rows corresponding to these voters. \n",
    "We then converted the `General_2008` variable to be numerical. \n",
    "\n",
    "When fitting our models in PySpark, we used `RFormula` to create a `features` column that is a vector of all relevant predictors (numerical and categorical)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91518760-88b2-4acd-93fa-984aa686d3cb",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a statistical method that allows us to estimate the probability that an event occurs, in this case, if an individual voted or not, given a set of independent variables $X_1, X_2,...,X_k$. If the $i$th individual has a probability $\\pi_i$ of voting, then the model is that the log odds of voting, $\\ln (\\frac{\\pi_i}{1-\\pi_i})$, can be expressed as a linear combination of the covariates $X_{i1}, X_{i2},...,X_{ik}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ln(\\frac{\\pi_i}{1-\\pi_i})&=\\beta_0+\\beta_1 X_{i1}+\\beta_2 X_{i2}+...\\beta_k X_{ik}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The coefficients of this function are $\\beta_0, \\beta_1, \\beta_2,\\dots, \\beta_k$, and indicate the relative effect of the corresponding variables $X_1,X_2,...,X_k$ on the response variable. The larger the coefficient $\\beta_j$, the more a varying value of $X_j$ changes the log-odds. The optimal coefficients maximize the function to find the best fit.\n",
    "\n",
    "Of course, this idea of logistic regression generalize to more than just voting; it applies whenever our concerned response variable is binary.\n",
    "\n",
    "Applying this to our data, we split our dataset into 70% training data and 30% testing data, and fit logistic regression on our training data. \n",
    "\n",
    "A plot of the beta coefficients can be seen in **Fig. 2** and a plot of the ranked coefficients can be seen in **Fig. 3**.\n",
    "\n",
    "<figure>\n",
    "<img src=\"...\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> Fig. 2: Logistic Regression Beta Coefficients </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"...\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> Fig. 3: Logistic Regression Ranked Coefficients </figcaption>\n",
    "</figure>\n",
    "\n",
    "The propensity score we obtain is [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6577fec-7b46-4a02-956d-81323467486f",
   "metadata": {},
   "source": [
    "## Model 1 - Logistic Regression on Voter Turnout\n",
    "\n",
    "For a first attempt in identitfying if Indiana's voter turnout in 2008 was affected by the strict voter ID law that was implemented during that year, we first fit a logistic regression to the voter turnout variable `General_2008`. In particular, we trained the model to predict whether a given individual voted during the 2008 election. However, the model was only given data from states without voter ID laws to learn from, i.e., all of our working states except for Indiana.\n",
    "\n",
    "The idea is that, if Indiana's law was key in shaping voter behavior of Indiana residents in 2008, then our model would have a poorer prediction accuracy when predicting voter turnout in Indiana than compared to a test set from the states without the same identification requirements.\n",
    "\n",
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4039a0-af05-4306-a2af-dfd83ce67514",
   "metadata": {},
   "source": [
    "The Figure below shows all of the predictor's with significantly large coefficients. I.e., predictors that have a significant role in predicting whether or not a given individual voted in the General 2008 election. Many coefficients had coefficients that were nearly zero, so the figure only displays predictors whose associated coefficient has a magnitude larger than 0.25. \n",
    "\n",
    "Perhaps unsurprisingly, it seems that an individual's party description plays a big role in their likelihood of voting. We see that more a lot of progressive or liberal-esque party description resulted in a lower odds of voting in the 2008 election. However, we see that those who identified with the \"Women's Equality party\" as well as African Americans had an increased odds of voting. This could be due to the historical implications the 2008 election had on the civil rights movement since it involved the first ever African American presidential candidate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d82c6e81-12dd-4ab3-8fda-7efdb301c412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"gs://voter-project-235-25/notebooks/jupyter/voter-project/download-1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"gs://voter-project-235-25/notebooks/jupyter/voter-project/download-1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b6e53-113c-43a9-acb8-438b2e2399c1",
   "metadata": {},
   "source": [
    "The summary of the performance metrics of the model is given in the table below\n",
    "\n",
    "| Metric | Other State | Indiana |\n",
    "| --- | --- | --- |\n",
    "| Overall Accuracy | 79.2% | 78.6% |\n",
    "| False Positive Rate | 13.3% | 14.9% |\n",
    "| False Negative Rate | 29.2% | 28.4% |\n",
    "| Area Under ROC | 86.2% | 86.0% |\n",
    "\n",
    "Overall, every metric between the control test set and the Indiana data are very comparable. It was verified that the model's performance was similar on the training data itself, indicating that overfitting was not an issue. \n",
    "\n",
    "Since the model could predict whether an individual from Indiana voted in the 2008 election just as well as it could predict someone voting in another state, it seems as though this voter ID law was potentially "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11734024-a001-421e-85de-a420cf7a5b1e",
   "metadata": {},
   "source": [
    "## Propensity Score\n",
    "Our goal is to predict if a voter has passed the law or not.\n",
    "\n",
    "We want to be able to find the probability if someone votes if they did not have the voter identification law implented.\n",
    "\n",
    "Assumption: People outside of Indiana are representative of the people in indiana.\n",
    "\n",
    "\n",
    "> Variables:\n",
    "> - T = whether they have the law\n",
    "> - Y = whether they voted in 2008\n",
    "> - P = predicted T\n",
    "\n",
    "To compare the voter data with and without the implementation of a voter identification law, we observe that the difference in means $$E[Y|T=1]-E[Y|T=0]$$ is [...]. Thus, the treated voters (with implementation of a voter identification law) have a [...] compared to non-treated voters.\n",
    "\n",
    "The propensity score is the conditional probability of receiving the treatment, the implementation of the voter identification law. Using this score means that we do not have to achieve conditional independence $(Y_1,Y_0) \\perp |X$. In other words, we do not have to condition on the whole $X$ to achieve independence of potential outcomes of the treatment. Instead, it is sufficient to control confounders $X$ for a propensity score $$P(x)=P(T|X)=E[T|X]$$ to achieve $(Y_1,Y_0) \\perp |P(x)$.\n",
    "\n",
    "The propensity score essentially converts $X$ into the treatment $T$, acting as a middleman between $X$ and $T$. Initially, we cannot compare treated and non-treated osbervations. However, we can compare a treated and a non-treated observation if they have the same probability of receiving the treatment since receiving or not receiving the treatment would be attributed to randomness. Thus, we hold the propensity score constant to make the data appear more random.\n",
    "\n",
    "\n",
    "### Propensity Weighting and Estimation\n",
    "\n",
    "We write the difference in means again, but we now condition on $X$: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& E[Y|X,T=1]-E[Y|X,T=0]\\\\\n",
    "& =E[\\frac{Y}{P(x)}|X,T=1]P(x)-E[\\frac{Y}{(1-P(x))}|X,T=0](1-P(x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In other words, the propensity score more heavily weights the observations with a low probability of receiving treatment, and weakly weights...\n",
    "\n",
    "We simplify our propensity score weighting estimator to $E[Y\\frac{T-P(x)}{P(x)(1-P(x)}]$, where $P(x)$ and $(1-P(x))$ both must be greater than $0$. Thus, each voter must have some probability of both receiving and not receiving the treatment of the implementation of a voter identification law.\n",
    "\n",
    "\n",
    "We now estimate the true propensity score $P(x)$ with $\\hat{P}(x)$ using logistic regression.\n",
    "\n",
    "> Issues\n",
    "> - The propensity score's strong predictive power can hurt our goals of causal inference.\n",
    ">> - We want out prediction to control for confounding variables, not neccesarily to predict the treatment very well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf3407-9697-4e57-a3a8-91f11602a054",
   "metadata": {},
   "source": [
    "## Doubly Robust Estimation\n",
    "\n",
    "So far we have used logistic regression to directly model voter turnout, and we have used propensity score weighting to estimate $E[Y|T=1] - E[Y|T=0] | X$ (i.e., the expected difference in liklihood for an Indianan vs other state resident to vote in the 2008). However, we can combine the advantages of both techniques into a single estimator known as the *doubly robust estimator*. The estimator for the average treatment effect is given by\n",
    "\n",
    "$$\\hat{ATE} = \\frac{1}{N}\\sum \\bigg( \\dfrac{T_i(Y_i - \\hat{\\mu_1}(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu_1}(X_i) \\bigg) - \\frac{1}{N}\\sum \\bigg( \\dfrac{(1-T_i)(Y_i - \\hat{\\mu_0}(X_i))}{1-\\hat{P}(X_i)} + \\hat{\\mu_0}(X_i) \\bigg)$$\n",
    "\n",
    "where $\\hat{\\mu_0}(x)$ is an estimate to $E[Y| X, T=0]$ (probability someone with given quality votes given they are from another state), $\\hat{\\mu_1}(x)$ is an estimate to $E[Y| X, T=1]$ (probability someone with given quality votes given they are from Indiana), and $\\hat{P}(x)$ is an estimation of the propensity score.\n",
    "\n",
    "In implementation, we can think of these quantities as follows:\n",
    "\n",
    "> - $\\hat{\\mu_0}(X_i)$: predicted probabilities of voting for all voters given a model is trained on other states.\n",
    "> - $\\hat{\\mu_1}(X_i)$: predicted probabilities of voting for all voters given a model is trained on just Indiana.\n",
    "> - $\\hat{P}(X_i)$: predicted probabilities of having a voter ID law for all voters given a model is trained on all states.\n",
    "\n",
    "In theory, this estimator would combine both the previous methods to obtain a number between -1 and 1 which would indicate the expected difference in probabilities between voters in Indiana and other states.\n",
    "\n",
    "Unfortunately, in implementation, we get a value of $\\hat{ATE} = -1.18$ which would indicate that someone in Indiana is expected to be 118% less likely to vote than someone from another state. Of course, this value is nonsensical, indicating that there was some sort of bug in the code most likely.\n",
    "\n",
    "This is likely, in part, due to the confusing conventions of PySparks `LogisticRegression` implementation. Typically, the probability $\\pi_i$ that gets predicted is interpreted as the probability that the true value is 1. It seems that PySpark implements this such that $\\pi_i$ is the probability the true value is 0. To add to this confusion, the output of a `LogisticRegression` model has a `probability` column which reports a vector containing both $\\pi_i$ and $1- \\pi_i$. Using either value gives a nonsensical $\\hat{ATE}$, but $-1.18$ is both the more sensical value as well as the value we are more confident was computed correctly.\n",
    "\n",
    "Unfortunately, due to time contrainsts, we are unable to resolve this problem before submission, but it was an interesting topic to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2c481-e077-4f9d-8692-88cf9d0c097d",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "[summary of everything]\n",
    "\n",
    "[issues - curse of dimensionality]\n",
    "\n",
    "[significance]\n",
    "\n",
    "[possible future work]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac20cc5-b91b-455b-be9e-11990075871c",
   "metadata": {},
   "source": [
    "## Resources\n",
    "https://www.ncsl.org/elections-and-campaigns/voter-id#undefined \n",
    "\n",
    "https://matheusfacure.github.io/python-causality-handbook/11-Propensity-Score.html\n",
    "\n",
    "https://www.ibm.com/topics/logistic-regression#:~:text=Resources-,What%20is%20logistic%20regression%3F,given%20dataset%20of%20independent%20variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63204e4b-8a00-47c5-992b-70d8c4cf06a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
>>>>>>> 8c0725f922c30e5157b6efc3510b40536e1582b3
