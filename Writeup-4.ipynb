{"cells": [{"cell_type": "markdown", "id": "a5b28054-4af3-4738-b79c-dd228e99c35f", "metadata": {}, "source": "# Analysis of Voter Turnout in Indiana Pre- and Post- Voter Identification Law\n### Authors: Christopher Lefrak, Hannah Li, George Yang, and Kuai Yu\n### PSTAT 235\n\nNOTES/TO-DO (delete this before submitting):\n- truncate/limit code so the writeup looks polished and professional (no raw outputs/errors)\n- interpret findings\n- include visualizations and graphs (EDA? theoretical concepts?)\n- run and write ONLY NECESSARY code (label all code clearly and eliminate commented out code) in the \"Final Code file.ipynb\" file, save all visualizations as images to embedd in this writeup document so it's more organized.\n- submission: \"Writeup.ipynb\" file and \"Final Code file.ipynb\"\n"}, {"cell_type": "markdown", "id": "821a2d3b-0c12-4ebc-a7db-91ad51fefd29", "metadata": {}, "source": "## Introduction\n\n[importance/potential effect of voter ID law]\n\nThirty-five of the fifty states of the U.S. have passed stricter voter ID laws that require or request voters to present a form of identification at the polls. \nThe remaining fifteen states do not require voters to present any documentation to vote at the polls. States such as Indiana, Wisconsin, and Tennessee have strict photo ID laws for voters, while states such as Minnesota, Nebraska, North Carolina, and Pennsylvania have no requirements for voter identification. A visualization of the levels of strictness of voter photo identification laws for each state can be seen in the map in **Fig. 1** below.\n\n<figure>\n<img src=\"https://drive.google.com/uc?export=view&id=1-bKVXaRl_j3trfCus6iWYylx3RX4ACPz\" style=\"width:100%\">\n<figcaption align = \"center\"> Fig. 1: Strictness Levels of US Voter ID Laws </figcaption>\n</figure>\n\nAdvantages of implementing stricter voter identification requirements include preventing voter impersonation, thus  increasing public confidence in election processes. Disadvantages of implementing stricter laws unnecessarily burdens voters and administrators.\n\n## Goals\nIn this project we focus our investigations of voter identification laws on the state of Indiana, which implemented a strict voter identification law in 2008. We seek to analyze how much voter turnout would have decreased or increased without the implentation of the law. \n\n> Project Goals\n> - Propensity score \n> - Conduct logistic regression\n> - Strengthen our PySpark data analysis skills, collaborative skills, and project organization skills"}, {"cell_type": "markdown", "id": "52ff4d48-f0d5-44ff-ad6c-638ef44b5a47", "metadata": {}, "source": "## Voter Data\n\n### Dataset Overview\n\nOur voter data is obtained from the course's VM2Uniform folder. We primarily use the dataset corresponding to Indiana. At a glance, the dataset contains 726 columns and 946908 rows, records beginning from .... and ending in 2021\n\n[eda/visualizations]\n\nWe subsetted the dataset to focus on a narrower set of voter attributes. The columns we selected from the original dataset can be seen in the following section.\n\n\n"}, {"cell_type": "markdown", "id": "706cd871-ea95-426f-9a37-369f076189f3", "metadata": {}, "source": "### Input Variables\nIn the table below we display the columns we keep from the original dataset\n\n| Column Name | Data Type | Values |\n| --- | --- | --- |\n| Voters_Gender | categorical | ... |\n| Voters_BirthDate | --- | ... |\n| Residence_Families_HHCount | numerical | 1 through 10 |\n| Residence_HHGender_Description | categorical | Cannot Determine, Female Only Household, Male Only Household, Mixed Gender Household |\n| Mailing_Families_HHCount | numerical |  ... |\n| Mailing_HHGender_Description | categorical |  ... |\n| Parties_Description | categorical | ... |\n| CommercialData_PropertyType | categorical | ... |\n| AddressDistricts_Change_Changed_CD | categorical | ... |\n| AddressDistricts_Change_Changed_SD | categorical | ... |\n| AddressDistricts_Change_Changed_HD | categorical | ... |\n| AddressDistricts_Change_Changed_County | categorical | ... |\n| Residence_Addresses_Density | numerical | ... |\n| CommercialData_EstimatedHHIncome | categorical | ... |\n| CommercialData_ISPSA | categorical | 0 through 9 |\n| CommercialData_AreaMedianEducationYears | numerical |  ... |\n| CommercialData_AreaMedianHousingValue | numerical | ... |\n| CommercialData_AreaPcntHHMarriedCoupleNoChild | categorical | ... |\n| CommercialData_AreaPcntHHMarriedCoupleWithChild | categorical | ... |\n| CommercialData_AreaPcntHHSpanishSpeaking | categorical | ... |\n| CommercialData_AreaPcntHHWithChildren | categorical | ... |\n| CommercialData_StateIncomeDecile | categorical | 0 through 9 |\n| EthnicGroups_EthnicGroup1Desc | categorical |  East and South Asian, Eurpoean, Hispanic and Portuguese, Likely African-American, N/A, Other |\n| CommercialData_DwellingType | categorical | Large mult wo/Apt number, POBOX, Single Family Dwelling Unit, Small Mult or large mult w/apt number |\n| CommercialData_PresenceOfChildrenCode | categorical | Known Data, Modeled Likely to have a child, Modeled Not as Likely to have a child, Not Likely to have a child |\n| CommercialData_DonatesToCharityInHome | categorical | Yes, unknown |\n| CommercialData_DwellingUnitSize | categorical | Single Family Dwelling Unit, Duplex, Triplex, 4, 5-9, 10-19, 20-49, 50-100, 101+ |\n| CommercialData_ComputerOwnerInHome | categorical | Yes, unknown |\n| CommercialData_DonatesEnvironmentCauseInHome | categorical | Yes, unknown |\n| CommercialData_Education | categorical | Unknown, HS Diploma - Extremely Likely, Some College -Extremely Likely, Bach Degree - Extremely Likely, Grad Degree - Extremely Likely, Less than HS Diploma - Ex Like, HS Diploma - Likely, Some College - Likely, Bach Degree - Likely, Grad Degree - Likely, Less than HS Diploma - Likely |\n\n\n### Other Variables\nThe table below shows other control variables that we expect to be highly associated with the response variable.\n\n| Column Name | Data Type | Values |\n| --- | --- | --- |\n| General_2000 | categorical |  |\n| General_2004 | categorical |  |\n| PresidentialPrimary_2000 | categorical | --- |\n| PresidentialPrimary_2004 | categorical |  |\n| General_2008 | categorical | --- |\n\nThe table below shows the response variable.\n\n| Column Name | Data Type | Values |\n| --- | --- | --- |\n| General_2008 | categorical |  |"}, {"cell_type": "markdown", "id": "50b2c95a-63d7-4dfc-9ced-29d69e89abb0", "metadata": {}, "source": "### Other States\nIn the table below are states that do not have strict voter identification laws.\n\n| State | Abbreviation |\n| --- | --- |\n| California | CA | \n| Illinois | IL |\n| Massachusetts | MA | \n| Maryland | MD | \n| Maine | ME |\n| Minnesota | MN | \n| North Carolina | NC | \n| Nebraska | NE |\n| New Jersey | NJ | \n| New Mexico | NM | \n| Nevada | NV |\n| New York | NY | \n| Oregon | OR | \n| Pennsylvania | PA |\n| Vermont | VT | \n"}, {"cell_type": "markdown", "id": "562385eb-ce20-4531-a9b4-136523d766c1", "metadata": {}, "source": "### Data Cleaning\n\nMany of the columns contain symbols including `$` and `%`, so we remove those symbols.\n\nMany columns are also missing data. In numerical columns, we impute these values in with the mean value to minimize any changes to z-scores of the given data. We encode categorical columns using PySpark's `StringIndexer`. This maps the categorical labels of a column a column of label indices ordered by frequencies of labels, where the most frequent label is assigned to index 0.\n\n\nOur data also contains many individuals who were not old enough to vote in the 2008 general election (they were below the 18 year old age requirement), so we removed the rows corresponding to these voters. \nWe then converted the `General_2008` variable to be numerical. \n"}, {"cell_type": "markdown", "id": "11734024-a001-421e-85de-a420cf7a5b1e", "metadata": {}, "source": "## Propensity Score\nOur goal is to predict if a voter has passed the law or not.\n\nWe want to be able to find the probability if someone votes if they did not have the voter identification law implented.\n\nAssumption: People outside of Indiana are representative of the people in indiana.\n\n\n> Variables:\n> - T = whether they have the law\n> - Y = whether they voted in 2008\n> - P = predicted T\n\nTo compare the voter data with and without the implementation of a voter identification law, we observe that the difference in means $$E[Y|T=1]-E[Y|T=0]$$ is [...]. Thus, the treated voters (with implementation of a voter identification law) have a [...] compared to non-treated voters.\n\nThe propensity score is the conditional probability of receiving the treatment, the implementation of the voter identification law. Using this score means that we do not have to achieve conditional independence $(Y_1,Y_0) \\perp |X$. In other words, we do not have to condition on the whole $X$ to achieve independence of potential outcomes of the treatment. Instead, it is sufficient to control confounders $X$ for a propensity score $$P(x)=P(T|X)=E[T|X]$$ to achieve $(Y_1,Y_0) \\perp |P(x)$.\n\nThe propensity score essentially converts $X$ into the treatment $T$, acting as a middleman between $X$ and $T$. Initially, we cannot compare treated and non-treated osbervations. However, we can compare a treated and a non-treated observation if they have the same probability of receiving the treatment since receiving or not receiving the treatment would be attributed to randomness. Thus, we hold the propensity score constant to make the data appear more random.\n\n\n### Propensity Weighting and Estimation\n\nWe write the difference in means again, but we now condition on $X$: \n\n$$\n\\begin{align}\n& E[Y|X,T=1]-E[Y|X,T=0]\\\\\n& =E[\\frac{Y}{P(x)}|X,T=1]P(x)-E[\\frac{Y}{(1-P(x))}|X,T=0](1-P(x))\n\\end{align}\n$$\n\nIn other words, the propensity score more heavily weights the observations with a low probability of receiving treatment, and weakly weights...\n\nWe simplify our propensity score weighting estimator to $E[Y\\frac{T-P(x)}{P(x)(1-P(x)}]$, where $P(x)$ and $(1-P(x))$ both must be greater than $0$. Thus, each voter must have some probability of both receiving and not receiving the treatment of the implementation of a voter identification law.\n\n\nWe now estimate the true propensity score $P(x)$ with $\\hat{P}(x)$ using logistic regression.\n\n> Issues\n> - The propensity score's strong predictive power can hurt our goals of causal inference.\n>> - We want out prediction to control for confounding variables, not neccesarily to predict the treatment very well."}, {"cell_type": "markdown", "id": "9abc2e49-f80b-4ba0-85a2-1b2a0a42e8bc", "metadata": {}, "source": "## Logistic Regression\n\nLogistic regression is a statistical method that allows us to estimate the probability that an event occurs, in this case, if an individual voted or not, given a set of independent variables $X_1, X_2,...X_k$. The function can be expressed in terms of the log odds, where the odds $\\frac{x}{1-x}$ are defined as the probability of success divided by the probability of failure:\n\n$$\n\\begin{align}\nlogit(x)&=\\frac{1}{1+e^{-x}} \\\\\nln(\\frac{x}{1-x})&=\\beta_0+\\beta_1 X_1+\\beta_2 X_2+...\\beta_k X_k\n\\end{align}\n$$\n\nThe coefficients of this function are $\\beta_0, \\beta_1, \\beta_2, \\beta_k$, and indicate the relative effect of the corresponding variables $X_1,X_2,...X_K$ on the response variable. The optimal coefficients maximize the function to find the best fit.\n\n\nApplying this to our data, we split our dataset into 70% training data and 30% testing data, and fit logistic regression on our training data. \n\nA plot of the beta coefficients can be seen in **Fig. 2** and a plot of the ranked coefficients can be seen in **Fig. 3**.\n\n<figure>\n<img src=\"...\" style=\"width:100%\">\n<figcaption align = \"center\"> Fig. 2: Logistic Regression Beta Coefficients </figcaption>\n</figure>\n\n\n<figure>\n<img src=\"...\" style=\"width:100%\">\n<figcaption align = \"center\"> Fig. 3: Logistic Regression Ranked Coefficients </figcaption>\n</figure>\n\nThe propensity score we obtain is [...]\n"}, {"cell_type": "markdown", "id": "64a2c481-e077-4f9d-8692-88cf9d0c097d", "metadata": {}, "source": "## Summary of Findings\n\n\n## Conclusion\n\n[summary of everything]\n\n[issues - curse of dimensionality]\n\n[significance]\n\n[possible future work]"}, {"cell_type": "markdown", "id": "2ac20cc5-b91b-455b-be9e-11990075871c", "metadata": {}, "source": "## Resources\nhttps://www.ncsl.org/elections-and-campaigns/voter-id#undefined \n\nhttps://matheusfacure.github.io/python-causality-handbook/11-Propensity-Score.html\n\nhttps://www.ibm.com/topics/logistic-regression#:~:text=Resources-,What%20is%20logistic%20regression%3F,given%20dataset%20of%20independent%20variables."}, {"cell_type": "code", "execution_count": null, "id": "63204e4b-8a00-47c5-992b-70d8c4cf06a8", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}